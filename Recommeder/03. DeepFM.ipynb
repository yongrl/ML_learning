{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "- [知乎](https://zhuanlan.zhihu.com/p/27999355)\n",
    "- [推荐系统遇上深度学习(三)--DeepFM模型理论和实践](https://www.jianshu.com/p/6f1c2643d31b)\n",
    "- [tensorflow-DeepFM](https://github.com/ChenglongChen/tensorflow-DeepFM/tree/master/example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepFM是一个集成了FM和DNN的神经网络框架，思路和Google的Wide&Deep相似，都包括wide和deep两部分。W&D模型的wide部分是广义线性模型，DeepFM的wide部分则是FM模型，两者的deep部分都是深度神经网络。DeepFM神经网络部分，隐含层的激活函数用ReLu和Tanh做信号非线性映射，Sigmoid函数做CTR预估的输出函数。\n",
    "\n",
    "W&D模型的输入向量维度很大，因为wide部分的特征包括了手工提取的pairwise特征组合，大大提高计算复杂度。和W&D模型相比，DeepFM的wide和deep部分共享相同的输入，可以提高训练效率，不需要额外的特征工程，用FM建模low-order的特征组合，用DNN建模high-order的特征组合，因此可以同时从raw feature中学习到low-和high-order的feature interactions。在真实应用市场的数据和criteo的数据集上实验验证，DeepFM在CTR预估的计算效率和AUC\n",
    "、LogLoss上超越了现有的模型（LR、FM、FNN、PNN、W&D）。\n",
    "\n",
    "market发现，一些order-2 interaction（如app category和time-stamp）、order-3 interaction（user gender、age和app category）都可以作为CTR的signal。因此，low-order和high-order的feature interactions都可以在CTR预估中发挥作用。但如何有效构建interactions呢？大部分的特征组合隐藏在数据中，难以发现关联关系，只能通过机器学习来自动挖掘。\n",
    "\n",
    "# 相关工作\n",
    "FTRL算法（McMahan et al. 2013），generalized linear model虽然简单，但实践中很有效果。但这类线性模型难以学习组合特征，一般需要手动构建特征向量，难以处理高阶组合或者没有出现在训练数据中的组合。\n",
    "\n",
    "Factorization Machine模型（Rendle\n",
    "2010），对特征之间进行向量内积，实现特征们的逐对组合pairwise interactions。理论上FM可以对高阶特征组合建模，但实践中只用order-2特征因为其高复杂度。\n",
    "\n",
    "DNN在特征表示学习中很有效果，可以用来学习组合特征。（Liu et al., 2015）和（Zhang et al., 2014）扩展CNN和RNN用于CTR预估，但CNN-based模型对领域特征有偏biased，RNN-based模型适合用在有时序依赖的点击数据。Factorization-machine supported Neural Network（FNN）（Zhang et al., 2016），在使用DNN前预训练FM，因此限制了FM的能力。Product-based Neural\n",
    "Network（PNN）（Qu et al., 2016）在DNN的embedding层和全连接层之间引入product层，来研究feature interactions。\n",
    "\n",
    "Wide & Deep模型(Cheng\n",
    "et al., 2016)认为，PNN/FNN和其他Deep模型提取很少low-order的feature interactions，提出的W&D模型可以同时对low-order和high-order建模，但要对wide和deep部分模型分别输入，其中wide部分还需要人工特征工程。\n",
    "![image.png](pic/DeepFM1.png)\n",
    "\n",
    "# 模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from time import time\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "from yellowfin import YFOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,feature_size,field_size, embedding_size = 8,\n",
    "                 dropout_fm = [1,1],deep_layers = [32,32],\n",
    "                 dropout_deep = [0.5,0.5,0.5],\n",
    "                 deep_layers_activation = tf.nn.relu,\n",
    "                 epoch = 10,batch_size = 256,\n",
    "                 learning_rate = 0.001, optimizer_type = \"adam\",\n",
    "                 batch_norm=0, batch_norm_decay=0.995,\n",
    "                 verbose=False, random_seed=2016,\n",
    "                 use_fm=True, use_deep=True,\n",
    "                 loss_type=\"logloss\", eval_metric=roc_auc_score,\n",
    "                 l2_reg=0.0, greater_is_better=True):\n",
    "        assert (use_fm or use_deep)\n",
    "        assert loss_type in [\"logloss\", \"mse\"], \\\n",
    "            \"loss_type can be either 'logloss' for classification task or 'mse' for regression task\"\n",
    "\n",
    "        self.feature_size = feature_size        # denote as M, size of the feature dictionary,one-hot之后的所有特征\n",
    "        self.field_size = field_size            # denote as F, size of the feature fields，未one-hot的所有特征维度，\n",
    "                                                # 对类别特征的每一个类别都做了一个因变量\n",
    "        self.embedding_size = embedding_size    # denote as K, size of the feature embedding\n",
    "\n",
    "        self.dropout_fm = dropout_fm\n",
    "        self.deep_layers = deep_layers\n",
    "        self.dropout_deep = dropout_deep\n",
    "        self.deep_layers_activation = deep_layers_activation\n",
    "        self.use_fm = use_fm\n",
    "        self.use_deep = use_deep\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer_type\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_decay = batch_norm_decay\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "        self.loss_type = loss_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.greater_is_better = greater_is_better\n",
    "        self.train_result, self.valid_result = [], []\n",
    "\n",
    "        self._init_graph()\n",
    "\n",
    "    def _init_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "\n",
    "            # input\n",
    "            # Xi: [[ind1_1, ind1_2, ...], [ind2_1, ind2_2, ...], ..., [indi_1, indi_2, ..., indi_j, ...], ...]\n",
    "            #indi_j is the feature index of feature field j of sample i in the dataset\n",
    "            # Xv: [[val1_1, val1_2, ...], [val2_1, val2_2, ...], ..., [vali_1, vali_2, ..., vali_j, ...], ...]\n",
    "            # vali_j is the feature value of feature field j of sample i in the dataset\n",
    "            # vali_j can be either binary (1/0, for binary/categorical features) or float (e.g., 10.24, for numerical features)\n",
    "            self.feat_index = tf.placeholder(tf.int32,shape=[None,None],name='feat_index') # None * F\n",
    "            self.feat_value = tf.placeholder(tf.float32,shape=[None,None],name='feat_value') # None * F\n",
    "            self.label = tf.placeholder(tf.float32,shape=[None,1],name='label')\n",
    "\n",
    "            self.dropout_keep_fm = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_fm\")\n",
    "            self.dropout_keep_deep = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_deep\")\n",
    "\n",
    "            self.train_phase = tf.placeholder(tf.bool,name='train_phase')\n",
    "\n",
    "            # Variable\n",
    "            self.weights = self._initialize_weights()\n",
    "\n",
    "            # model\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'],self.feat_index)\n",
    "            feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])\n",
    "            self.embeddings = tf.multiply(self.embeddings,feat_value) # None*F*K\n",
    "\n",
    "\n",
    "            # first-order term\n",
    "            self.y_first_order = tf.nn.embedding_lookup(self.weights[\"feature_bias\"],self.feat_index) # None*F*1\n",
    "            self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order,feat_value),2)# None*G\n",
    "            self.y_first_order = tf.nn.dropout(self.y_first_order,self.dropout_fm[0])\n",
    "\n",
    "\n",
    "            # second-order term\n",
    "            # sum_square part\n",
    "            self.summed_features_emb = tf.reduce_sum(self.embeddings, 1)  # None * K\n",
    "            self.summed_features_emb_square = tf.square(self.summed_features_emb)  # None * K\n",
    "\n",
    "            # square_sum part\n",
    "            self.squared_features_emb = tf.square(self.embeddings)\n",
    "            self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, 1)  # None * K\n",
    "\n",
    "            # second order\n",
    "            # 0.5*sum((sum(vx))**2-sum(v**2*x**2))\n",
    "            self.y_second_order = 0.5 * tf.subtract(self.summed_features_emb_square, self.squared_sum_features_emb)  # None * K\n",
    "            self.y_second_order = tf.nn.dropout(self.y_second_order, self.dropout_keep_fm[1])  # None * K\n",
    "\n",
    "            # 与fm不同，first_order 和second_order是通过concat联在一起的\n",
    "            # self.fm = tf.add(self.y_first_order,self.y_second_order)\n",
    "\n",
    "            # Deep\n",
    "            self.y_deep = tf.reshape(self.embeddings,shape=[-1,self.field_size*self.embedding_size]) # None*(F*K)\n",
    "            # embedding dropout\n",
    "            self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[0])\n",
    "            for i in range(0,len(self.deep_layers)):\n",
    "                self.y_deep = tf.add(tf.matmul(self.y_deep,self.weights['layer_%d'%i]),self.weights['bias_%d' %i])# None * layer[i] * 1\n",
    "                if self.batch_norm:\n",
    "                    self.y_deep = self.batch_norm_layer(self.y_deep,train_phase=self.train_phase,scope_bn=\"bn_%d\" %i) # None * layer[i] * 1\n",
    "                self.y_deep = self.deep_layers_activation(self.y_deep)\n",
    "                self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[i+1])\n",
    "\n",
    "            # DeepFM\n",
    "            if self.use_fm and self.use_deep:\n",
    "                input = tf.concat([self.y_first_order,self.y_second_order,self.y_deep],axis=1)\n",
    "            elif self.use_fm:\n",
    "                input = tf.concat([self.y_first_order,self.y_second_order],axis=1)\n",
    "            elif self.use_deep:\n",
    "                input = self.y_deep\n",
    "            self.out = tf.add(tf.matmul(input,self.weights['concat_projection']),self.weights[\"concat_bias\"])\n",
    "\n",
    "            # loss\n",
    "            if self.loss_type == \"logloss\":\n",
    "                self.out = tf.nn.sigmoid(self.out)\n",
    "                self.loss = tf.losses.log_loss(self.label,self.out)\n",
    "            elif self.loss_type == 'mse':\n",
    "                self.loss = tf.nn.l2_loss(tf.subtract(self.label,self.out))\n",
    "\n",
    "            # l2\n",
    "            if self.l2_reg > 0:\n",
    "                self.loss += tf.contrib.layers.l2_regularizer(self.l2_reg)(self.weights[\"concat_projection\"])\n",
    "                if self.use_deep:\n",
    "                    for i in range(len(self.deep_layers)):\n",
    "                        self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                            self.l2_reg)(self.weights[\"layer_%d\"%i])\n",
    "\n",
    "            # optimizer\n",
    "            if self.optimizer_type == 'adam':\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate,\n",
    "                                                        beta1=0.9,\n",
    "                                                        beta2=0.999,\n",
    "                                                        epsilon=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == 'adagrad':\n",
    "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n",
    "                                                           initial_accumulator_value=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == 'gd':\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate,\n",
    "                                                                   ).minimize(self.loss)\n",
    "            elif self.optimizer_type =='momentum':\n",
    "                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate,\n",
    "                                                            momentum=0.95).minimize(self.loss)\n",
    "            elif self.optimizer_type == 'yellowfin':\n",
    "                self.optimizer = YFOptimizer(learning_rate=self.learning_rate,momentum=0.0).minimize(self.loss)\n",
    "\n",
    "            # init\n",
    "            self.saver = tf.train.Saver()\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = self._init_session()\n",
    "            self.sess.run(init)\n",
    "\n",
    "            # number of params\n",
    "            total_parameters = 0\n",
    "            for variable in self.weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            if self.verbose > 0:\n",
    "                print(\"#params: %d\" % total_parameters)\n",
    "\n",
    "    def _init_session(self):\n",
    "        #config = tf.ConfigProto(device_count={\"gpu\":0})\n",
    "        #config.gpu_options.allow_growth = True\n",
    "        return  tf.Session()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        weights = dict()\n",
    "\n",
    "        # embeddings\n",
    "        weights[\"feature_embeddings\"] = tf.Variable(tf.random_normal([self.feature_size,self.embedding_size],0,0.01),\n",
    "                                                    name='feature_embeddings') # feature_embedding: f*K\n",
    "\n",
    "        weights[\"feature_bias\"] = tf.Variable(tf.random_uniform([self.feature_size,1],0.0,1.0),\n",
    "                                              name='feature_bias')\n",
    "\n",
    "\n",
    "        # deep layers\n",
    "        # number of layer of hidden dnn\n",
    "        num_layer = len(self.deep_layers)\n",
    "        input_size = self.field_size * self.embedding_size\n",
    "\n",
    "        #Glorot and Bengio (2010) 建议使用标准初始化（normalized initialization）\n",
    "        glorot = np.sqrt(2.0 / (input_size+self.deep_layers[0]))\n",
    "        weights[\"layer_0\"] = tf.Varible(np.random.normal(loc=0,scale=glorot,\n",
    "                                                         size=(input_size,self.deep_layers[0])),dtype=np.float32)\n",
    "        weights['bias_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,\n",
    "                                                         size=(1,self.deep_layers[0])),dtype=tf.float32)\n",
    "\n",
    "        for i in range(1,num_layer):\n",
    "            glorot = np.sqrt(2.0 / (self.deep_layers[i-1]+self.deep_layers[i]))\n",
    "            weights['layer_%d'%i] = tf.Varible(np.random.normal(loc=0,scale=glorot,\n",
    "                                                             size=(self.deep_layers[i-1],self.deep_layers[i])),dtype=np.float32)\n",
    "            weights['bias_%d' %i] = tf.Variable(np.random.normal(loc=0,scale=glorot,\n",
    "                                                             size=(1,self.deep_layers[i])),dtype=tf.float32)\n",
    "\n",
    "\n",
    "        # final concat projection layer\n",
    "        if self.use_fm and self.use_deep:\n",
    "            input_size = self.field_size + self.embedding_size + self.deep_layers[-1]\n",
    "        elif self.use_fm:\n",
    "            input_size = self.field_size + self.embedding_size\n",
    "        elif self.use_deep:\n",
    "            input_size = self.deep_layers[-1]\n",
    "\n",
    "        weights[\"concat_projection\"] = tf.Variable(\n",
    "            np.random.normal(loc=0, scale=glorot, size=(input_size, 1)),\n",
    "            dtype=np.float32)  # layers[i-1]*layers[i]\n",
    "        weights[\"concat_bias\"] = tf.Variable(tf.constant(0.01), dtype=np.float32)\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def batch_norm_layer(self,x,train_phase,scope_bn):\n",
    "        '''\n",
    "        批标准化所要解决的问题是：模型参数在学习阶段的变化，会使每个隐藏层输出的分布也发生改变。这意味着靠后的层要在训练过程中去适应这些变化。\n",
    "        为了解决这个问题，论文BN2015提出了批标准化，即在训练时作用于每个神经元激活函数（比如sigmoid或者ReLU函数）的输入，\n",
    "        使得基于每个批次的训练样本，激活函数的输入都能满足均值为0，方差为1的分布。\n",
    "        :param x:\n",
    "        :param train_phase:\n",
    "        :param scope_bn:\n",
    "        :return:\n",
    "        '''\n",
    "        bn_train = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                              is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
    "        bn_inference = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                  is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
    "\n",
    "        #tf.cond()类似于c语言中的if...else...，用来控制数据流向，但是仅仅类似而已，其中差别还是挺大的。关于tf.cond（）函数的具体操作，我参考了tf的说明文档。\n",
    "        # format：tf.cond(pred, fn1, fn2, name=None)\n",
    "        z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "        return z\n",
    "\n",
    "    def get_batch(self,Xi,Xv,y,batch_size,index):\n",
    "        start = index * batch_size\n",
    "        end = (index+1)*batch_size\n",
    "        end = end if end<len(y) else len(y)\n",
    "        return Xi[start:end],Xv[start:end],[[y_] for y_ in y[start:end]]\n",
    "\n",
    "    # shuffle three lists simutaneously\n",
    "    def shuffle_in_unison_scary(self,a,b,c):\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.ranfom.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.ranfom.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "\n",
    "    def fit_on_batch(self, Xi, Xv, y):\n",
    "        feed_dict = {self.feat_index: Xi,\n",
    "                     self.feat_value: Xv,\n",
    "                     self.label: y,\n",
    "                     self.dropout_keep_fm: self.dropout_fm,\n",
    "                     self.dropout_keep_deep: self.dropout_deep,\n",
    "                     self.train_phase: True}\n",
    "        loss, opt = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "    def fit(self, Xi_train, Xv_train, y_train,\n",
    "            Xi_valid=None, Xv_valid=None, y_valid=None,\n",
    "            early_stopping=False, refit=False):\n",
    "        \"\"\"\n",
    "        :param Xi_train: [[ind1_1, ind1_2, ...], [ind2_1, ind2_2, ...], ..., [indi_1, indi_2, ..., indi_j, ...], ...]\n",
    "                         indi_j is the feature index of feature field j of sample i in the training set\n",
    "        :param Xv_train: [[val1_1, val1_2, ...], [val2_1, val2_2, ...], ..., [vali_1, vali_2, ..., vali_j, ...], ...]\n",
    "                         vali_j is the feature value of feature field j of sample i in the training set\n",
    "                         vali_j can be either binary (1/0, for binary/categorical features) or float (e.g., 10.24, for numerical features)\n",
    "        :param y_train: label of each sample in the training set\n",
    "        :param Xi_valid: list of list of feature indices of each sample in the validation set\n",
    "        :param Xv_valid: list of list of feature values of each sample in the validation set\n",
    "        :param y_valid: label of each sample in the validation set\n",
    "        :param early_stopping: perform early stopping or not\n",
    "        :param refit: refit the model on the train+valid dataset or not\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        has_valid = Xv_valid is not None\n",
    "        for epoch in range(self.epoch):\n",
    "            t1= time()\n",
    "            self.shuffle_in_unison_scary(Xi_train,Xv_train,y_train)\n",
    "            total_batch = int(len(y_train)/self.batch_size)\n",
    "            for i in range(total_batch):\n",
    "                Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train, self.batch_size, i)\n",
    "                self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "\n",
    "            # evaluate training and validation datasets in this epoch\n",
    "            train_result = self.evaluate(Xi_train,Xv_train,y_train)\n",
    "            self.train_result.append(train_result)\n",
    "\n",
    "            if has_valid:\n",
    "                valid_result = self.evaluate(Xi_train,Xv_train,y_train)\n",
    "                self.valid_result.append(train_result)\n",
    "\n",
    "            if self.verbose > 0 and epoch % self.verbose == 0:\n",
    "                if has_valid:\n",
    "                    print(\"[%d] train-result=%.4f, valid-result=%.4f [%.1f s]\"\n",
    "                          % (epoch + 1, train_result, valid_result, time() - t1))\n",
    "                else:\n",
    "                    print(\"[%d] train-result=%.4f [%.1f s]\"\n",
    "                          % (epoch + 1, train_result, time() - t1))\n",
    "            if has_valid and early_stopping and self.training_termination(self.valid_result):\n",
    "                break\n",
    "\n",
    "        # fit a few more epoch on train+valid until result reaches the best_train_score\n",
    "        if has_valid and refit:\n",
    "            if self.greater_is_better:\n",
    "                best_valid_score = max(self.valid_result)\n",
    "            else:\n",
    "                best_valid_score = min(self.valid_result)\n",
    "            best_epoch = self.valid_result.index(best_valid_score)\n",
    "            best_train_score = self.train_result[best_epoch]\n",
    "            Xi_train = Xi_train + Xi_valid\n",
    "            Xv_train = Xv_train + Xv_valid\n",
    "            y_train = y_train + y_valid\n",
    "            for epoch in range(100):\n",
    "                self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "                total_batch = int(len(y_train) / self.batch_size)\n",
    "                for i in range(total_batch):\n",
    "                    Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train,\n",
    "                                                                 self.batch_size, i)\n",
    "                    self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "                # check\n",
    "                train_result = self.evaluate(Xi_train, Xv_train, y_train)\n",
    "                if abs(train_result - best_train_score) < 0.001 or \\\n",
    "                        (self.greater_is_better and train_result > best_train_score) or \\\n",
    "                        ((not self.greater_is_better) and train_result < best_train_score):\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self,Xi,Xv):\n",
    "        \"\"\"\n",
    "       :param Xi: list of list of feature indices of each sample in the dataset\n",
    "       :param Xv: list of list of feature values of each sample in the dataset\n",
    "       :return: predicted probability of each sample\n",
    "       \"\"\"\n",
    "        # dummy y\n",
    "        dummy_y = [1] * len(Xi)\n",
    "        batch_index = 0\n",
    "        Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "        y_pred = None\n",
    "        while len(Xi_batch) > 0:\n",
    "            num_batch = len(y_batch)\n",
    "            feed_dict = {self.feat_index: Xi_batch,\n",
    "                         self.feat_value: Xv_batch,\n",
    "                         self.label: y_batch,\n",
    "                         self.dropout_keep_fm: [1.0] * len(self.dropout_fm),\n",
    "                         self.dropout_keep_deep: [1.0] * len(self.dropout_deep),\n",
    "                         self.train_phase: False}\n",
    "            batch_out = self.sess.run(self.out, feed_dict=feed_dict)\n",
    "    \n",
    "            if batch_index == 0:\n",
    "                y_pred = np.reshape(batch_out, (num_batch,))\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, np.reshape(batch_out, (num_batch,))))\n",
    "    \n",
    "            batch_index += 1\n",
    "            Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "    \n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def training_termination(self, valid_result):\n",
    "        if len(valid_result) > 5:\n",
    "            if self.greater_is_better:\n",
    "                if valid_result[-1] < valid_result[-2] and \\\n",
    "                        valid_result[-2] < valid_result[-3] and \\\n",
    "                        valid_result[-3] < valid_result[-4] and \\\n",
    "                        valid_result[-4] < valid_result[-5]:\n",
    "                    return True\n",
    "            else:\n",
    "                if valid_result[-1] > valid_result[-2] and \\\n",
    "                        valid_result[-2] > valid_result[-3] and \\\n",
    "                        valid_result[-3] > valid_result[-4] and \\\n",
    "                        valid_result[-4] > valid_result[-5]:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def evaluate(self, Xi, Xv, y):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :param y: label of each sample in the dataset\n",
    "        :return: metric of the evaluation\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(Xi, Xv)\n",
    "        return self.eval_metric(y, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
