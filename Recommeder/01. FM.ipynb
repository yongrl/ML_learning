{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorization Machine\n",
    "\n",
    "参考：\n",
    "- [推荐系统遇上深度学习(一)--FM模型理论和实践](https://blog.csdn.net/jiangjiang_jian/article/details/80630873)\n",
    "- [分解机(Factorization Machines)推荐算法原理](https://www.cnblogs.com/pinard/p/6370127.html)\n",
    "\n",
    "\n",
    "推荐系统对商品的点击率进行预估建模时，特征处理环节，除了单特征的处理外，通常要对特征进行组合，对于特征组合来说，常用的两种方法为FM系列和Tree系列。 \n",
    "\n",
    "\n",
    "\n",
    "一般的线性模型为\n",
    "$$y= w_0 + \\sum_{i=1}^n w_i x_i$$\n",
    "\n",
    "从表达式中也可以看出，一般的线性模型没有考虑任何特征之间的联系。\n",
    "\n",
    "在考虑特征两个特征的组合时，可以用下面的二项式形式表示\n",
    "\n",
    "$$y= w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^{n-1} \\ sum_{j = i+1}^n w_{ij}x_i x_j$$\n",
    "\n",
    "FM模型比一般的多项式模型就多了二项式的部分。\n",
    "\n",
    "但是，在模型求解释，很多特征的组合数据量非常小，甚至是没有的，特别是特征对特征进行了one hot方法的处理之后，这样组合特征的参数就没有办法求参，因此在求参时，也使用了类似因变量分解的方法：\n",
    "\n",
    "\n",
    "![FM1](pic/FM1.png)\n",
    "![FM2](pic/FM2.png)\n",
    "![FM3](pic/FM3.png)\n",
    "\n",
    "\n",
    "\n",
    "# 将列表转换为稀疏矩阵\n",
    "从列表创建稀疏矩阵\n",
    "参数：\n",
    "- dic: 特征列表地点，key值为特征名\n",
    "- ix: \n",
    "- p: 特征维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr\n",
    "\n",
    "def vectorize_dic(dic,ix=None,p=None,n=0,g=0):\n",
    "    \"\"\"\n",
    "    dic -- dictionary of feature lists. Keys are the name of features\n",
    "    ix -- index generator (default None)\n",
    "    p -- dimension of featrure space (number of columns in the sparse matrix) (default None)\n",
    "    \"\"\"\n",
    "    if ix==None:\n",
    "        ix = dict()\n",
    " \n",
    "    nz = n * g\n",
    " \n",
    "    col_ix = np.empty(nz,dtype = int)\n",
    " \n",
    "    i = 0\n",
    "    for k,lis in dic.items():\n",
    "        for t in range(len(lis)):\n",
    "            ix[str(lis[t]) + str(k)] = ix.get(str(lis[t]) + str(k),0) + 1\n",
    "            col_ix[i+t*g] = ix[str(lis[t]) + str(k)]\n",
    "        i += 1\n",
    " \n",
    "    row_ix = np.repeat(np.arange(0,n),g)\n",
    "    data = np.ones(nz)\n",
    "    if p == None:\n",
    "        p = len(ix)\n",
    " \n",
    "    ixx = np.where(col_ix < p)\n",
    "    return csr.csr_matrix((data[ixx],(row_ix[ixx],col_ix[ixx])),shape=(n,p)),ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# laod data with pandas\n",
    "cols = ['user', 'item', 'rating', 'timestamp']\n",
    "train = pd.read_csv('http://files.grouplens.org/datasets/movielens/ml-100k/ua.base', delimiter='\\t', names=cols)\n",
    "test = pd.read_csv('http://files.grouplens.org/datasets/movielens/ml-100k/ua.test', delimiter='\\t', names=cols)\n",
    "\n",
    "# vectorize data and convert them to csr matrix\n",
    "x_train,ix = vectorize_dic({'users':train['user'].values,\n",
    "                            'items':train['item'].values},n=len(train.index),g=2)\n",
    " \n",
    " \n",
    "x_test,ix = vectorize_dic({'users':test['user'].values,\n",
    "                           'items':test['item'].values},ix,x_train.shape[1],n=len(test.index),g=2)\n",
    " \n",
    " \n",
    "y_train = train['rating'].values\n",
    "y_test = test['rating'].values\n",
    " \n",
    "x_train = x_train.todense()\n",
    "x_test = x_test.todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义FM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "n, p = x_train.shape\n",
    "# # of factor\n",
    "k = 10\n",
    "X = tf.placeholder('float',[None,p])\n",
    "y = tf.placeholder('float',[None,1])\n",
    "\n",
    "# bias and weigths\n",
    "w0 = tf.Variable(tf.zeros([1]))\n",
    "w = tf.Variable(tf.zeros([p]))\n",
    "\n",
    "V = tf.Variable(tf.random_normal([k,p],mean = 0,stddev = 0.01))\n",
    "\n",
    "# y的估计量\n",
    "y_hat = tf.Variable(tf.zeros([n,10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义y的计算公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\hat{y}(\\mathbf{x}) = w_0 + \\sum_{j=1}^{p}w_jx_j + \\frac{1}{2} \\sum_{f=1}^{k} ((\\sum_{j=1}^{p}v_{j,f}x_j)^2-\\sum_{j=1}^{p}v_{j,f}^2 x_j^2)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "display(Math(r'\\hat{y}(\\mathbf{x}) = w_0 + \\sum_{j=1}^{p}w_jx_j + \\frac{1}{2} \\sum_{f=1}^{k} ((\\sum_{j=1}^{p}v_{j,f}x_j)^2-\\sum_{j=1}^{p}v_{j,f}^2 x_j^2)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0318 14:39:09.256244  7336 deprecation.py:506] From <ipython-input-5-23a1bb815007>:2: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    " # 线性部分\n",
    "linear_terms = tf.add(w0,tf.reduce_sum(tf.multiply(w,X),1,keep_dims=True))\n",
    "# 组合特征部分\n",
    "pair_interactions = 0.5 * tf.reduce_sum(\n",
    "         tf.subtract(\n",
    "        tf.pow(tf.matmul(X,tf.transpose(V)),2),\n",
    "        tf.matmul(tf.pow(X,2),tf.transpose(tf.pow(V,2))))\n",
    "      ,axis = 1 , keep_dims=True)\n",
    "\n",
    "y_hat = tf.add(linear_terms,pair_interactions)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义损失函数\n",
    "损失函数基本是一致的，但是$\\hat y$在各个模型中的定义不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle L = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda_w ||W||^2 + \\lambda_v ||V||^2$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'L = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda_w ||W||^2 + \\lambda_v ||V||^2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularized sum of squares loss function over W and V\n",
    "lambda_w = tf.constant(0.001, name='lambda_w')\n",
    "lambda_v = tf.constant(0.001, name='lambda_v')\n",
    "\n",
    "l2_norm = (tf.reduce_sum(\n",
    "            tf.add(\n",
    "                tf.multiply(lambda_w, tf.pow(w , 2)),\n",
    "                tf.multiply(lambda_v, tf.pow(w, 2)))))\n",
    "\n",
    "error = tf.reduce_mean(tf.square(tf.subtract(y, y_hat)))\n",
    "loss = tf.add(error, l2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\Theta_{i+1} = \\Theta_{i} - \\eta \\frac{\\delta L}{\\delta \\Theta}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'\\Theta_{i+1} = \\Theta_{i} - \\eta \\frac{\\delta L}{\\delta \\Theta}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据批处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(X_, y_=None, batch_size=-1):\n",
    "    n_samples = X_.shape[0]\n",
    "\n",
    "    if batch_size == -1:\n",
    "        batch_size = n_samples\n",
    "    if batch_size < 1:\n",
    "       raise ValueError('Parameter batch_size={} is unsupported'.format(batch_size))\n",
    "\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        upper_bound = min(i + batch_size, n_samples)\n",
    "        ret_x = X_[i:upper_bound]\n",
    "        ret_y = None\n",
    "        if y_ is not None:\n",
    "            ret_y = y_[i:i + batch_size]\n",
    "            yield (ret_x, ret_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开启tensorflow并训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.666977\n",
      "13.459277\n",
      "11.895468\n",
      "12.229354\n",
      "11.978907\n",
      "11.697511\n",
      "10.898112\n",
      "10.752986\n",
      "10.290718\n",
      "9.813786\n",
      "9.807136\n",
      "9.5477705\n",
      "9.0315695\n",
      "8.111083\n",
      "8.3757\n",
      "7.897618\n",
      "7.987485\n",
      "7.2583404\n",
      "7.2744007\n",
      "6.8828506\n",
      "6.7953267\n",
      "6.2840652\n",
      "6.292376\n",
      "6.002656\n",
      "5.5803113\n",
      "5.7798123\n",
      "5.6434474\n",
      "5.336013\n",
      "5.000166\n",
      "4.937323\n",
      "4.778964\n",
      "4.803615\n",
      "4.535076\n",
      "4.5546055\n",
      "4.4564857\n",
      "4.26334\n",
      "4.0320787\n",
      "4.000592\n",
      "3.7109952\n",
      "3.6614008\n",
      "3.6036034\n",
      "3.701316\n",
      "3.7156446\n",
      "3.391756\n",
      "3.3318903\n",
      "2.9437358\n",
      "3.1904266\n",
      "3.0303473\n",
      "3.0058758\n",
      "2.781895\n",
      "2.7252507\n",
      "2.7902694\n",
      "2.8194306\n",
      "2.6593657\n",
      "2.5745451\n",
      "2.5639713\n",
      "2.5399594\n",
      "2.496343\n",
      "2.4049273\n",
      "2.3603868\n",
      "2.2856984\n",
      "2.2806635\n",
      "2.183684\n",
      "2.0733094\n",
      "2.128459\n",
      "2.1011772\n",
      "2.0609765\n",
      "1.9393634\n",
      "1.9505033\n",
      "1.9703048\n",
      "1.8953605\n",
      "1.8605076\n",
      "1.8673614\n",
      "1.7454628\n",
      "1.8196458\n",
      "1.7379729\n",
      "1.7835699\n",
      "1.7304085\n",
      "1.7636402\n",
      "1.7235775\n",
      "1.6770111\n",
      "1.6989435\n",
      "1.7189751\n",
      "1.6492547\n",
      "1.7453214\n",
      "1.6447566\n",
      "1.6604309\n",
      "1.6269314\n",
      "1.6431615\n",
      "1.6631123\n",
      "1.5384884\n",
      "1.6034584\n",
      "1.4558429\n",
      "1.5337511\n",
      "1.4424423\n",
      "1.5770731\n",
      "1.5240046\n",
      "1.4888039\n",
      "1.4044237\n",
      "1.40075\n",
      "1.4411513\n",
      "1.524299\n",
      "1.4657338\n",
      "1.4918414\n",
      "1.4985902\n",
      "1.3611166\n",
      "1.4735907\n",
      "1.4240189\n",
      "1.4301275\n",
      "1.4217443\n",
      "1.4535977\n",
      "1.3570567\n",
      "1.3617817\n",
      "1.3055319\n",
      "1.3638569\n",
      "1.4010793\n",
      "1.3124095\n",
      "1.2943324\n",
      "1.415459\n",
      "1.3885963\n",
      "1.2944738\n",
      "1.3986586\n",
      "1.3821502\n",
      "1.3542874\n",
      "1.3158569\n",
      "1.2827415\n",
      "1.3609928\n",
      "1.3827343\n",
      "1.3542538\n",
      "1.341381\n",
      "1.3505592\n",
      "1.2895464\n",
      "1.3213104\n",
      "1.3078183\n",
      "1.3248036\n",
      "1.3334421\n",
      "1.3064864\n",
      "1.2386125\n",
      "1.2707701\n",
      "1.3290595\n",
      "1.2342613\n",
      "1.269129\n",
      "1.2817198\n",
      "1.3771985\n",
      "1.3333951\n",
      "1.3616174\n",
      "1.2487301\n",
      "1.3034712\n",
      "1.3100587\n",
      "1.2120212\n",
      "1.3163521\n",
      "1.3873941\n",
      "1.3221991\n",
      "1.3557148\n",
      "1.1898718\n",
      "1.2585208\n",
      "1.3132057\n",
      "1.254173\n",
      "1.2459699\n",
      "1.2544496\n",
      "1.3444397\n",
      "1.3101602\n",
      "1.3024446\n",
      "1.2672645\n",
      "1.3274679\n",
      "1.2897413\n",
      "1.2960771\n",
      "1.337715\n",
      "1.3453816\n",
      "1.2584494\n",
      "1.3075514\n",
      "1.3567711\n",
      "1.326808\n",
      "1.3001945\n",
      "1.2399579\n",
      "1.2294822\n",
      "1.3021163\n",
      "1.202866\n",
      "1.2418704\n",
      "1.3863211\n",
      "1.2747165\n",
      "1.1802354\n",
      "1.2666464\n",
      "1.220086\n",
      "1.2612766\n",
      "1.2997777\n",
      "1.3110611\n",
      "1.2776436\n",
      "1.2774934\n",
      "1.3246671\n",
      "1.2433537\n",
      "1.3079865\n",
      "1.2727005\n",
      "1.2574492\n",
      "1.3050742\n",
      "1.216718\n",
      "1.2539701\n",
      "1.3359858\n",
      "1.3269718\n",
      "1.2652003\n",
      "1.3662397\n",
      "1.2150306\n",
      "1.1760793\n",
      "1.2936076\n",
      "1.3144715\n",
      "1.2733657\n",
      "1.2739516\n",
      "1.2990314\n",
      "1.3252559\n",
      "1.2132144\n",
      "1.2751437\n",
      "1.1700621\n",
      "1.3410678\n",
      "1.2558043\n",
      "1.26381\n",
      "1.2803589\n",
      "1.3729059\n",
      "1.2507981\n",
      "1.295742\n",
      "1.2818147\n",
      "1.215847\n",
      "1.1976374\n",
      "1.2458918\n",
      "1.2509283\n",
      "1.2503111\n",
      "1.3359463\n",
      "1.268566\n",
      "1.3197968\n",
      "1.3010015\n",
      "1.2494311\n",
      "1.1807034\n",
      "1.3583269\n",
      "1.354898\n",
      "1.3169042\n",
      "1.1904912\n",
      "1.365908\n",
      "1.2969356\n",
      "1.2486244\n",
      "1.2347692\n",
      "1.2918257\n",
      "1.2739483\n",
      "1.2084633\n",
      "1.2690275\n",
      "1.3461145\n",
      "1.2815937\n",
      "1.2600014\n",
      "1.1846377\n",
      "1.2621483\n",
      "1.1775845\n",
      "1.2397515\n",
      "1.2522686\n",
      "1.2904669\n",
      "1.2565298\n",
      "1.3538574\n",
      "1.2672346\n",
      "1.2970754\n",
      "1.2275673\n",
      "1.2703152\n",
      "1.2161157\n",
      "1.2304269\n",
      "1.2629414\n",
      "1.3262361\n",
      "1.2364324\n",
      "1.2358769\n",
      "1.1884077\n",
      "1.273589\n",
      "1.2812432\n",
      "1.2668631\n",
      "1.2462972\n",
      "1.2350852\n",
      "1.3176965\n",
      "1.286709\n",
      "1.3235383\n",
      "1.2581143\n",
      "1.2045345\n",
      "1.3011144\n",
      "1.306703\n",
      "1.2652383\n",
      "1.3373915\n",
      "1.1925575\n",
      "1.318936\n",
      "1.30839\n",
      "1.2562634\n",
      "1.3326025\n",
      "1.2592366\n",
      "1.2330776\n",
      "1.2354022\n",
      "1.321073\n",
      "1.2728424\n",
      "1.2186663\n",
      "1.2432861\n",
      "1.2430767\n",
      "1.3282232\n",
      "1.2202966\n",
      "1.156873\n",
      "1.3114866\n",
      "1.192095\n",
      "1.2543316\n",
      "1.2789962\n",
      "1.3012389\n",
      "1.2508032\n",
      "1.2616651\n",
      "1.288294\n",
      "1.3133197\n",
      "1.2733535\n",
      "1.2780162\n",
      "1.2748247\n",
      "1.2999599\n",
      "1.3419217\n",
      "1.2769924\n",
      "1.274068\n",
      "1.240624\n",
      "1.2464519\n",
      "1.3082609\n",
      "1.1882058\n",
      "1.2630329\n",
      "1.2379102\n",
      "1.2255626\n",
      "1.2897968\n",
      "1.309176\n",
      "1.3382549\n",
      "1.297677\n",
      "1.2763414\n",
      "1.194121\n",
      "1.2757312\n",
      "1.2210886\n",
      "1.2680825\n",
      "1.286605\n",
      "1.2909876\n",
      "1.2203534\n",
      "1.2590277\n",
      "1.2081132\n",
      "1.3669473\n",
      "1.2662572\n",
      "1.2703248\n",
      "1.2509495\n",
      "1.3034781\n",
      "1.2102692\n",
      "1.3028009\n",
      "1.2821072\n",
      "1.3145143\n",
      "1.2454351\n",
      "1.2362843\n",
      "1.3541622\n",
      "1.3116748\n",
      "1.3091582\n",
      "1.3051876\n",
      "1.3201026\n",
      "1.239676\n",
      "1.2996757\n",
      "1.2395078\n",
      "1.3294394\n",
      "1.2338012\n",
      "1.2164495\n",
      "1.2485484\n",
      "1.2292372\n",
      "1.2733505\n",
      "1.2618213\n",
      "1.2660953\n",
      "1.2215507\n",
      "1.2016948\n",
      "1.2877231\n",
      "1.2524965\n",
      "1.2683228\n",
      "1.1652652\n",
      "1.2419528\n",
      "1.2055886\n",
      "1.4143108\n",
      "1.2231882\n",
      "1.3402634\n",
      "1.2788793\n",
      "1.3787537\n",
      "1.3051003\n",
      "1.1946687\n",
      "1.3346573\n",
      "1.2197398\n",
      "1.2857034\n",
      "1.2716855\n",
      "1.2115655\n",
      "1.3010217\n",
      "1.2997178\n",
      "1.2874693\n",
      "1.3338721\n",
      "1.286408\n",
      "1.1847292\n",
      "1.2954588\n",
      "1.2463988\n",
      "1.2830436\n",
      "1.1756685\n",
      "1.2505932\n",
      "1.334419\n",
      "1.265502\n",
      "1.2965002\n",
      "1.2502548\n",
      "1.2363722\n",
      "1.2640932\n",
      "1.2189953\n",
      "1.2028177\n",
      "1.3245741\n",
      "1.2322801\n",
      "1.2890555\n",
      "1.2356563\n",
      "1.2611808\n",
      "1.260411\n",
      "1.193782\n",
      "1.2856505\n",
      "1.2876643\n",
      "1.2440585\n",
      "1.1583688\n",
      "1.2654573\n",
      "1.3071153\n",
      "1.2470359\n",
      "1.2320678\n",
      "1.2599534\n",
      "1.3497305\n",
      "1.292689\n",
      "1.2186891\n",
      "1.2848395\n",
      "1.2649403\n",
      "1.281893\n",
      "1.1292044\n",
      "1.259197\n",
      "1.3096015\n",
      "1.2863289\n",
      "1.3044782\n",
      "1.2920378\n",
      "1.2312385\n",
      "1.2821099\n",
      "1.358351\n",
      "1.2240075\n",
      "1.2776242\n",
      "1.174015\n",
      "1.2243959\n",
      "1.286625\n",
      "1.3205737\n",
      "1.3003716\n",
      "1.3032353\n",
      "1.2671663\n",
      "1.2985864\n",
      "1.2609289\n",
      "1.2225409\n",
      "1.2580087\n",
      "1.2127602\n",
      "1.258005\n",
      "1.2025036\n",
      "1.2961028\n",
      "1.4037324\n",
      "1.2841256\n",
      "1.3415914\n",
      "1.347919\n",
      "1.2421956\n",
      "1.2641771\n",
      "1.3310183\n",
      "1.2751545\n",
      "1.1487242\n",
      "1.2483314\n",
      "1.2360154\n",
      "1.2946886\n",
      "1.3659122\n",
      "1.3315312\n",
      "1.3005633\n",
      "1.2047168\n",
      "1.279025\n",
      "1.325152\n",
      "1.3175571\n",
      "1.2394487\n",
      "1.3335145\n",
      "1.3013281\n",
      "1.2766733\n",
      "1.3118212\n",
      "1.2668582\n",
      "1.3423954\n",
      "1.3336253\n",
      "1.2576648\n",
      "1.2875794\n",
      "1.2686026\n",
      "1.2198662\n",
      "1.1930684\n",
      "1.2258285\n",
      "1.2828038\n",
      "1.2759826\n",
      "1.2692453\n",
      "1.3416654\n",
      "1.3299167\n",
      "1.2196208\n",
      "1.2154471\n",
      "1.2673194\n",
      "1.2116741\n",
      "1.3108752\n",
      "1.2455099\n",
      "1.3083107\n",
      "1.2035317\n",
      "1.2308731\n",
      "1.2625766\n",
      "1.2674612\n",
      "1.3008947\n",
      "1.2680703\n",
      "1.3090137\n",
      "1.2949193\n",
      "1.2323538\n",
      "1.2215964\n",
      "1.2887691\n",
      "1.2203968\n",
      "1.2350315\n",
      "1.208284\n",
      "1.2161779\n",
      "1.2248565\n",
      "1.2514966\n",
      "1.2266304\n",
      "1.2901534\n",
      "1.3234636\n",
      "1.3011235\n",
      "1.2757281\n",
      "1.2782377\n",
      "1.240911\n",
      "1.3259445\n",
      "1.2324702\n",
      "1.2229737\n",
      "1.3747832\n",
      "1.3509704\n",
      "1.2600609\n",
      "1.277707\n",
      "1.2556467\n",
      "1.2879473\n",
      "1.3146442\n",
      "1.2305608\n",
      "1.2463562\n",
      "1.2029884\n",
      "1.2905709\n",
      "1.2552264\n",
      "1.2030253\n",
      "1.3282171\n",
      "1.1969327\n",
      "1.2386017\n",
      "1.2231854\n",
      "1.229551\n",
      "1.2289981\n",
      "1.3222095\n",
      "1.2490478\n",
      "1.3006235\n",
      "1.3329686\n",
      "1.2190493\n",
      "1.2501156\n",
      "1.2032659\n",
      "1.1950502\n",
      "1.2840303\n",
      "1.3980643\n",
      "1.2966878\n",
      "1.2685391\n",
      "1.2861562\n",
      "1.2435758\n",
      "1.3339382\n",
      "1.2265501\n",
      "1.3944167\n",
      "1.296011\n",
      "1.3203726\n",
      "1.1844139\n",
      "1.1888788\n",
      "1.3075742\n",
      "1.2944672\n",
      "1.2725877\n",
      "1.1738333\n",
      "1.3213161\n",
      "1.206413\n",
      "1.1765181\n",
      "1.2809168\n",
      "1.2565894\n",
      "1.2531286\n",
      "1.3091612\n",
      "1.2824308\n",
      "1.287333\n",
      "1.2446043\n",
      "1.2695994\n",
      "1.2055997\n",
      "1.1904757\n",
      "1.3762965\n",
      "1.2937554\n",
      "1.2926348\n",
      "1.2946318\n",
      "1.1512061\n",
      "1.1779349\n",
      "1.3383809\n",
      "1.3567469\n",
      "1.2638142\n",
      "1.2136692\n",
      "1.2523545\n",
      "1.2942005\n",
      "1.2789413\n",
      "1.3195909\n",
      "1.3470896\n",
      "1.2429855\n",
      "1.2548279\n",
      "1.3335922\n",
      "1.3432361\n",
      "1.268861\n",
      "1.2106693\n",
      "1.2210277\n",
      "1.2885002\n",
      "1.3106105\n",
      "1.1985717\n",
      "1.2576696\n",
      "1.2900801\n",
      "1.3299009\n",
      "1.2795415\n",
      "1.2974461\n",
      "1.240305\n",
      "1.3091484\n",
      "1.2645729\n",
      "1.2629781\n",
      "1.2768323\n",
      "1.1688123\n",
      "1.2320102\n",
      "1.1925168\n",
      "1.2645496\n",
      "1.3117894\n",
      "1.2298912\n",
      "1.261296\n",
      "1.2714593\n",
      "1.2341273\n",
      "1.3003137\n",
      "1.2129483\n",
      "1.2849113\n",
      "1.1852357\n",
      "1.251386\n",
      "1.3468236\n",
      "1.2237734\n",
      "1.2356006\n",
      "1.2406646\n",
      "1.3560835\n",
      "1.2511876\n",
      "1.2188677\n",
      "1.2072567\n",
      "1.2270755\n",
      "1.3104293\n",
      "1.2088585\n",
      "1.2808766\n",
      "1.2903457\n",
      "1.2832023\n",
      "1.2481431\n",
      "1.2266425\n",
      "1.2579137\n",
      "1.2863514\n",
      "1.2919669\n",
      "1.2899741\n",
      "1.1752872\n",
      "1.2297175\n",
      "1.2766271\n",
      "1.1405662\n",
      "1.2036031\n",
      "1.2363946\n",
      "1.2483841\n",
      "1.2826699\n",
      "1.2756119\n",
      "1.2309684\n",
      "1.3459558\n",
      "1.1997849\n",
      "1.3395422\n",
      "1.237156\n",
      "1.2858386\n",
      "1.1963196\n",
      "1.328292\n",
      "1.2719064\n",
      "1.229743\n",
      "1.2401806\n",
      "1.2545094\n",
      "1.1630034\n",
      "1.2219348\n",
      "1.3167807\n",
      "1.2791206\n",
      "1.3002992\n",
      "1.2815107\n",
      "1.2449434\n",
      "1.3589404\n",
      "1.2649609\n",
      "1.215159\n",
      "1.2581447\n",
      "1.2239821\n",
      "1.2394931\n",
      "1.2793615\n",
      "1.2423589\n",
      "1.2792572\n",
      "1.3824229\n",
      "1.2706522\n",
      "1.2200032\n",
      "1.2498262\n",
      "1.2643874\n",
      "1.2202033\n",
      "1.2752923\n",
      "1.2553474\n",
      "1.3554263\n",
      "1.2743707\n",
      "1.327073\n",
      "1.2700412\n",
      "1.2586137\n",
      "1.3180683\n",
      "1.2668246\n",
      "1.2343918\n",
      "1.3509295\n",
      "1.3161677\n",
      "1.237011\n",
      "1.253507\n",
      "1.3418151\n",
      "1.303381\n",
      "1.3098435\n",
      "1.1931064\n",
      "1.2560807\n",
      "1.3035263\n",
      "1.3197297\n",
      "1.2090111\n",
      "1.2134067\n",
      "1.3131164\n",
      "1.2228435\n",
      "1.3318884\n",
      "1.1550808\n",
      "1.3181123\n",
      "1.2487304\n",
      "1.2245928\n",
      "1.252734\n",
      "1.3939202\n",
      "1.2900058\n",
      "1.2583854\n",
      "1.3315166\n",
      "1.2757776\n",
      "1.3096241\n",
      "1.2943248\n",
      "1.2301248\n",
      "1.1986691\n",
      "1.2648027\n",
      "1.1926112\n",
      "1.2268715\n",
      "1.2523253\n",
      "1.2573367\n",
      "1.2480558\n",
      "1.096672\n",
      "1.2343516\n",
      "1.2318307\n",
      "1.2836357\n",
      "1.2384173\n",
      "1.315462\n",
      "1.2000953\n",
      "1.3066494\n",
      "1.170327\n",
      "1.3092548\n",
      "1.3754424\n",
      "1.3026687\n",
      "1.298779\n",
      "1.2739488\n",
      "1.2256428\n",
      "1.337071\n",
      "1.2631137\n",
      "1.2802497\n",
      "1.2835572\n",
      "1.2986488\n",
      "1.2803416\n",
      "1.23537\n",
      "1.311809\n",
      "1.346118\n",
      "1.2740015\n",
      "1.2453262\n",
      "1.2899207\n",
      "1.3085341\n",
      "1.1844997\n",
      "1.2558867\n",
      "1.292605\n",
      "1.1800059\n",
      "1.3878367\n",
      "1.3129576\n",
      "1.120423\n",
      "1.2742525\n",
      "1.227999\n",
      "1.3447059\n",
      "1.3176671\n",
      "1.2523396\n",
      "1.3000768\n",
      "1.2763299\n",
      "1.2613646\n",
      "1.3127327\n",
      "1.2195295\n",
      "1.3204681\n",
      "1.2756506\n",
      "1.3006775\n",
      "1.3089731\n",
      "1.3339545\n",
      "1.1334002\n",
      "1.2702264\n",
      "1.2594328\n",
      "1.3813493\n",
      "1.1628399\n",
      "1.2462618\n",
      "1.2875042\n",
      "1.1958421\n",
      "1.3276538\n",
      "1.2784559\n",
      "1.2453213\n",
      "1.3426867\n",
      "1.1588056\n",
      "1.2873315\n",
      "1.289374\n",
      "1.2322848\n",
      "1.2146894\n",
      "1.2464428\n",
      "1.2940509\n",
      "1.228246\n",
      "1.2057452\n",
      "1.2117186\n",
      "1.2089666\n",
      "1.2563154\n",
      "1.3082838\n",
      "1.3310381\n",
      "1.2782573\n",
      "1.2217032\n",
      "1.2477541\n",
      "1.2581122\n",
      "1.3149768\n",
      "1.2361392\n",
      "1.3445356\n",
      "1.2833894\n",
      "1.3082472\n",
      "1.2464571\n",
      "1.270065\n",
      "1.2489084\n",
      "1.2292448\n",
      "1.2371495\n",
      "1.2441157\n",
      "1.257794\n",
      "1.3709921\n",
      "1.414229\n",
      "1.1719522\n",
      "1.2430471\n",
      "1.3022952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2281094\n",
      "1.2608694\n",
      "1.254743\n",
      "1.2039205\n",
      "1.2427069\n",
      "1.2659601\n",
      "1.2707117\n",
      "1.2360288\n",
      "1.2394352\n",
      "1.2804197\n",
      "1.3515017\n",
      "1.304545\n",
      "1.2484348\n",
      "1.4070529\n",
      "1.2600833\n",
      "1.2306561\n",
      "1.2972014\n",
      "1.2733343\n",
      "1.2691842\n",
      "1.3705152\n",
      "1.1961622\n",
      "1.2656883\n",
      "1.2471582\n",
      "1.3337793\n",
      "1.2540843\n",
      "1.3906453\n",
      "1.2499503\n",
      "1.3154749\n",
      "1.2571304\n",
      "1.3174648\n",
      "1.3138297\n",
      "1.2979499\n",
      "1.1781074\n",
      "1.2801539\n",
      "1.3292255\n",
      "1.31469\n",
      "1.2696134\n",
      "1.159064\n",
      "1.1563606\n",
      "1.2419709\n",
      "1.2854751\n",
      "1.2241461\n",
      "1.3326541\n",
      "1.1675496\n",
      "1.1252637\n",
      "1.1865844\n",
      "1.2870222\n",
      "1.2709103\n",
      "1.3779898\n",
      "1.1960359\n",
      "1.2970524\n",
      "1.2550465\n",
      "1.3419309\n",
      "1.3319819\n",
      "1.2115636\n",
      "1.3045535\n",
      "1.3031638\n",
      "1.3097249\n",
      "1.2326785\n",
      "1.309197\n",
      "1.2528201\n",
      "1.2576752\n",
      "1.294744\n",
      "1.1927041\n",
      "1.3005142\n",
      "1.251434\n",
      "1.3150468\n",
      "1.3097295\n",
      "1.2267739\n",
      "1.2047822\n",
      "1.2299514\n",
      "1.2610465\n",
      "1.276422\n",
      "1.2183188\n",
      "1.1934199\n",
      "1.2522523\n",
      "1.1634479\n",
      "1.3176885\n",
      "1.1678786\n",
      "1.2911397\n",
      "\n",
      "[1.266807]\n",
      "1.1255252\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 1000\n",
    " \n",
    "# Launch the graph\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    " \n",
    "    for epoch in tqdm(range(epochs), unit='epoch'):\n",
    "        perm = np.random.permutation(x_train.shape[0])\n",
    "        # iterate over batches\n",
    "        for bX, bY in batcher(x_train[perm], y_train[perm], batch_size):\n",
    "            _,t = sess.run([optimizer,loss], feed_dict={X: bX.reshape(-1, p), y: bY.reshape(-1, 1)})\n",
    "            print(t)\n",
    " \n",
    " \n",
    "    errors = []\n",
    "    for bX, bY in batcher(x_test, y_test):\n",
    "        errors.append(sess.run(error, feed_dict={X: bX.reshape(-1, p), y: bY.reshape(-1, 1)}))\n",
    "        print(errors)\n",
    "    RMSE = np.sqrt(np.array(errors).mean())\n",
    "    print (RMSE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
