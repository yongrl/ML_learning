{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "博客：[推荐系统遇上深度学习(六)--PNN模型理论和实践](https://blog.csdn.net/jiangjiang_jian/article/details/80674157)\n",
    "\n",
    "论文：[Product-based Neural Networks for User Response\n",
    "Prediction，2016](https://arxiv.org/pdf/1611.00144.pdf)\n",
    "\n",
    "在CTR预估中，为了解决稀疏特征的问题，学者们提出了FM模型来建模特征之间的交互关系。但是FM模型只能表达特征之间两两组合之间的关系，无法建模两个特征之间深层次的关系或者说多个特征之间的交互关系，因此学者们通过Deep Network来建模更高阶的特征之间的关系。\n",
    "\n",
    "因此 FM和深度网络DNN的结合也就成为了CTR预估问题中主流的方法。有关FM和DNN的结合有两种主流的方法，并行结构和串行结构。两种结构的理解以及实现如下表所示：\n",
    "\n",
    "|结构\t|描述\t|常见模型|\n",
    "|------|-----|------|\n",
    "|并行结构\t| FM部分和DNN部分分开计算，只在输出层进行一次融合得到结果\t|DeepFM，DCN，Wide&Deep|\n",
    "|串行结构\t| 将FM的一次项和二次项结果(或其中之一)作为DNN部分的输入，经DNN得到最终结果\t|PNN,NFM,AFM|\n",
    "\n",
    "这章讲第一个串行结构PNN.\n",
    "\n",
    "PNN，全称为Product-based Neural Networks，是一种基于乘法运算来提现特征交叉的dnn模型。其网络结构如下图所示：\n",
    "![image.png](pic/PNN1.jpg)\n",
    "从整体结构来看，PNN与FNN的区别在于多了一层Product Layer。PNN的结构为embeddding+product layer + fcs。而PNN使用product的方式做特征交叉的想法是认为在ctr场景中，特征的交叉更加提现在一种“且”的关系下，而add的操作，是一种“或”的关系，所以product的形式更加合适，会有更好的效果。Product Layer相当于给FM初始化的特征embedding做了一次交叉操作，且不需要像FNN一样需要预训练FM的embedding 参数。\n",
    "![image.png](pic/PNN1.png)\n",
    "![image.png](pic/PNN2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class PNN(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, feature_size, field_size,\n",
    "                 embedding_size=8,\n",
    "                 deep_layers=[32, 32], deep_init_size = 50,\n",
    "                 dropout_deep=[0.5, 0.5, 0.5],\n",
    "                 deep_layer_activation=tf.nn.relu,\n",
    "                 epoch=10, batch_size=256,\n",
    "                 learning_rate=0.001, optimizer=\"adam\",\n",
    "                 batch_norm=0, batch_norm_decay=0.995,\n",
    "                 verbose=False, random_seed=2016,\n",
    "                 loss_type=\"logloss\", eval_metric=roc_auc_score,\n",
    "                greater_is_better=True,\n",
    "                 use_inner=True):\n",
    "        assert loss_type in [\"logloss\", \"mse\"], \\\n",
    "            \"loss_type can be either 'logloss' for classification task or 'mse' for regression task\"\n",
    "\n",
    "        self.feature_size = feature_size\n",
    "        self.field_size = field_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.deep_layers = deep_layers\n",
    "        self.deep_init_size = deep_init_size\n",
    "        self.dropout_dep = dropout_deep\n",
    "        self.deep_layers_activation = deep_layer_activation\n",
    "\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_decay = batch_norm_decay\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "        self.loss_type = loss_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.greater_is_better = greater_is_better\n",
    "        self.train_result,self.valid_result = [],[]\n",
    "\n",
    "        self.use_inner = use_inner\n",
    "\n",
    "        self._init_graph()\n",
    "\n",
    "    def _init_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "\n",
    "            self.feat_index = tf.placeholder(tf.int32,\n",
    "                                             shape=[None,None],\n",
    "                                             name='feat_index')\n",
    "            self.feat_value = tf.placeholder(tf.float32,\n",
    "                                           shape=[None,None],\n",
    "                                           name='feat_value')\n",
    "\n",
    "            self.label = tf.placeholder(tf.float32,shape=[None,1],name='label')\n",
    "            self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name='dropout_deep_deep')\n",
    "            self.train_phase = tf.placeholder(tf.bool,name='train_phase')\n",
    "\n",
    "            self.weights = self._initialize_weights()\n",
    "\n",
    "            # Embeddings\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'],self.feat_index) # N * F * K\n",
    "            feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])\n",
    "            self.embeddings = tf.multiply(self.embeddings,feat_value) # N * F * K\n",
    "\n",
    "\n",
    "            # Linear Singal\n",
    "            linear_output = []\n",
    "            for i in range(self.deep_init_size):\n",
    "                linear_output.append(tf.reshape(\n",
    "                    tf.reduce_sum(tf.multiply(self.embeddings,self.weights['product-linear'][i]),axis=[1,2]),shape=(-1,1)))# N * 1\n",
    "\n",
    "            self.lz = tf.concat(linear_output,axis=1) # N * init_deep_size\n",
    "\n",
    "            # Quardatic Singal\n",
    "            quadratic_output = []\n",
    "            if self.use_inner:\n",
    "                for i in range(self.deep_init_size):\n",
    "                    theta = tf.multiply(self.embeddings,tf.reshape(self.weights['product-quadratic-inner'][i],(1,-1,1))) # N * F * K\n",
    "                    quadratic_output.append(tf.reshape(tf.norm(tf.reduce_sum(theta,axis=1),axis=1),shape=(-1,1))) # N * 1\n",
    "\n",
    "            else:\n",
    "                embedding_sum = tf.reduce_sum(self.embeddings,axis=1)\n",
    "                p = tf.matmul(tf.expand_dims(embedding_sum,2),tf.expand_dims(embedding_sum,1)) # N * K * K\n",
    "                for i in range(self.deep_init_size):\n",
    "                    theta = tf.multiply(p,tf.expand_dims(self.weights['product-quadratic-outer'][i],0)) # N * K * K\n",
    "                    quadratic_output.append(tf.reshape(tf.reduce_sum(theta,axis=[1,2]),shape=(-1,1))) # N * 1\n",
    "\n",
    "            self.lp = tf.concat(quadratic_output,axis=1) # N * init_deep_size\n",
    "\n",
    "            self.y_deep = tf.nn.relu(tf.add(tf.add(self.lz, self.lp), self.weights['product-bias']))\n",
    "            self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[0])\n",
    "\n",
    "\n",
    "            # Deep component\n",
    "            for i in range(0,len(self.deep_layers)):\n",
    "                self.y_deep = tf.add(tf.matmul(self.y_deep,self.weights[\"layer_%d\" %i]), self.weights[\"bias_%d\"%i])\n",
    "                self.y_deep = self.deep_layers_activation(self.y_deep)\n",
    "                self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[i+1])\n",
    "\n",
    "\n",
    "\n",
    "            self.out = tf.add(tf.matmul(self.y_deep,self.weights['output']),self.weights['output_bias'])\n",
    "\n",
    "            # loss\n",
    "            if self.loss_type == \"logloss\":\n",
    "                self.out = tf.nn.sigmoid(self.out)\n",
    "                self.loss = tf.losses.log_loss(self.label, self.out)\n",
    "            elif self.loss_type == \"mse\":\n",
    "                self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))\n",
    "\n",
    "\n",
    "\n",
    "            if self.optimizer_type == \"adam\":\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,\n",
    "                                                        epsilon=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"adagrad\":\n",
    "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n",
    "                                                           initial_accumulator_value=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"gd\":\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"momentum\":\n",
    "                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize(\n",
    "                    self.loss)\n",
    "\n",
    "\n",
    "            #init\n",
    "            self.saver = tf.train.Saver()\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(init)\n",
    "\n",
    "            # number of params\n",
    "            total_parameters = 0\n",
    "            for variable in self.weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            if self.verbose > 0:\n",
    "                print(\"#params: %d\" % total_parameters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        weights = dict()\n",
    "\n",
    "        #embeddings\n",
    "        weights['feature_embeddings'] = tf.Variable(\n",
    "            tf.random_normal([self.feature_size,self.embedding_size],0.0,0.01),\n",
    "            name='feature_embeddings')\n",
    "        weights['feature_bias'] = tf.Variable(tf.random_normal([self.feature_size,1],0.0,1.0),name='feature_bias')\n",
    "\n",
    "\n",
    "        #Product Layers\n",
    "        if self.use_inner:\n",
    "            weights['product-quadratic-inner'] = tf.Variable(tf.random_normal([self.deep_init_size,self.field_size],0.0,0.01))\n",
    "        else:\n",
    "            weights['product-quadratic-outer'] = tf.Variable(\n",
    "                tf.random_normal([self.deep_init_size, self.embedding_size,self.embedding_size], 0.0, 0.01))\n",
    "\n",
    "\n",
    "\n",
    "        weights['product-linear'] = tf.Variable(tf.random_normal([self.deep_init_size,self.field_size,self.embedding_size],0.0,0.01))\n",
    "        weights['product-bias'] = tf.Variable(tf.random_normal([self.deep_init_size,],0,0,1.0))\n",
    "        #deep layers\n",
    "        num_layer = len(self.deep_layers)\n",
    "        input_size = self.deep_init_size\n",
    "        glorot = np.sqrt(2.0/(input_size + self.deep_layers[0]))\n",
    "\n",
    "        weights['layer_0'] = tf.Variable(\n",
    "            np.random.normal(loc=0,scale=glorot,size=(input_size,self.deep_layers[0])),dtype=np.float32\n",
    "        )\n",
    "        weights['bias_0'] = tf.Variable(\n",
    "            np.random.normal(loc=0,scale=glorot,size=(1,self.deep_layers[0])),dtype=np.float32\n",
    "        )\n",
    "\n",
    "\n",
    "        for i in range(1,num_layer):\n",
    "            glorot = np.sqrt(2.0 / (self.deep_layers[i - 1] + self.deep_layers[i]))\n",
    "            weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i - 1], self.deep_layers[i])),\n",
    "                dtype=np.float32)  # layers[i-1] * layers[i]\n",
    "            weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),\n",
    "                dtype=np.float32)  # 1 * layer[i]\n",
    "\n",
    "\n",
    "        glorot = np.sqrt(2.0/(input_size + 1))\n",
    "        weights['output'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(self.deep_layers[-1],1)),dtype=np.float32)\n",
    "        weights['output_bias'] = tf.Variable(tf.constant(0.01),dtype=np.float32)\n",
    "\n",
    "\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def get_batch(self,Xi,Xv,y,batch_size,index):\n",
    "        start = index * batch_size\n",
    "        end = (index + 1) * batch_size\n",
    "        end = end if end < len(y) else len(y)\n",
    "        return Xi[start:end],Xv[start:end],[[y_] for y_ in y[start:end]]\n",
    "\n",
    "    # shuffle three lists simutaneously\n",
    "    def shuffle_in_unison_scary(self, a, b, c):\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "\n",
    "    def predict(self, Xi, Xv,y):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :return: predicted probability of each sample\n",
    "        \"\"\"\n",
    "        # dummy y\n",
    "        feed_dict = {self.feat_index: Xi,\n",
    "                     self.feat_value: Xv,\n",
    "                     self.label: y,\n",
    "                     self.dropout_keep_deep: [1.0] * len(self.dropout_dep),\n",
    "                     self.train_phase: True}\n",
    "\n",
    "        loss = self.sess.run([self.loss], feed_dict=feed_dict)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def fit_on_batch(self,Xi,Xv,y):\n",
    "        feed_dict = {self.feat_index:Xi,\n",
    "                     self.feat_value:Xv,\n",
    "                     self.label:y,\n",
    "                     self.dropout_keep_deep:self.dropout_dep,\n",
    "                     self.train_phase:True}\n",
    "\n",
    "        loss,opt = self.sess.run([self.loss,self.optimizer],feed_dict=feed_dict)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fit(self, Xi_train, Xv_train, y_train,\n",
    "            Xi_valid=None, Xv_valid=None, y_valid=None,\n",
    "            early_stopping=False, refit=False):\n",
    "        \"\"\"\n",
    "        :param Xi_train: [[ind1_1, ind1_2, ...], [ind2_1, ind2_2, ...], ..., [indi_1, indi_2, ..., indi_j, ...], ...]\n",
    "                         indi_j is the feature index of feature field j of sample i in the training set\n",
    "        :param Xv_train: [[val1_1, val1_2, ...], [val2_1, val2_2, ...], ..., [vali_1, vali_2, ..., vali_j, ...], ...]\n",
    "                         vali_j is the feature value of feature field j of sample i in the training set\n",
    "                         vali_j can be either binary (1/0, for binary/categorical features) or float (e.g., 10.24, for numerical features)\n",
    "        :param y_train: label of each sample in the training set\n",
    "        :param Xi_valid: list of list of feature indices of each sample in the validation set\n",
    "        :param Xv_valid: list of list of feature values of each sample in the validation set\n",
    "        :param y_valid: label of each sample in the validation set\n",
    "        :param early_stopping: perform early stopping or not\n",
    "        :param refit: refit the model on the train+valid dataset or not\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        has_valid = Xv_valid is not None\n",
    "        for epoch in range(self.epoch):\n",
    "            t1 = time()\n",
    "            self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "            total_batch = int(len(y_train) / self.batch_size)\n",
    "            for i in range(total_batch):\n",
    "                Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train, self.batch_size, i)\n",
    "                self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "\n",
    "            if has_valid:\n",
    "                y_valid = np.array(y_valid).reshape((-1,1))\n",
    "                loss = self.predict(Xi_valid, Xv_valid, y_valid)\n",
    "                print(\"epoch\",epoch,\"loss\",loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"data/DCN_train.csv\"\n",
    "TEST_FILE = \"data/DCN_test.csv\"\n",
    "\n",
    "SUB_DIR = \"output\"\n",
    "\n",
    "\n",
    "NUM_SPLITS = 3\n",
    "RANDOM_SEED = 2017\n",
    "\n",
    "# types of columns of the dataset dataframe\n",
    "CATEGORICAL_COLS = [\n",
    "    'ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat',\n",
    "    'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat',\n",
    "    'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat',\n",
    "    'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat',\n",
    "    'ps_car_10_cat', 'ps_car_11_cat',\n",
    "]\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    # # binary\n",
    "    # \"ps_ind_06_bin\", \"ps_ind_07_bin\", \"ps_ind_08_bin\",\n",
    "    # \"ps_ind_09_bin\", \"ps_ind_10_bin\", \"ps_ind_11_bin\",\n",
    "    # \"ps_ind_12_bin\", \"ps_ind_13_bin\", \"ps_ind_16_bin\",\n",
    "    # \"ps_ind_17_bin\", \"ps_ind_18_bin\",\n",
    "    # \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\",\n",
    "    # \"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\",\n",
    "    # numeric\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\",\n",
    "    \"ps_car_12\", \"ps_car_13\", \"ps_car_14\", \"ps_car_15\",\n",
    "\n",
    "    # feature engineering\n",
    "    \"missing_feat\", \"ps_car_13_x_ps_reg_03\",\n",
    "]\n",
    "\n",
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\",\n",
    "    \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\",\n",
    "    \"ps_calc_09\", \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\",\n",
    "    \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\",\n",
    "    \"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class FeatureDictionary(object):\n",
    "    def __init__(self,trainfile=None,testfile=None,\n",
    "                 dfTrain=None,dfTest=None,numeric_cols=[],\n",
    "                 ignore_cols=[]):\n",
    "        assert not ((trainfile is None) and (dfTrain is None)), \"trainfile or dfTrain at least one is set\"\n",
    "        assert not ((trainfile is not None) and (dfTrain is not None)), \"only one can be set\"\n",
    "        assert not ((testfile is None) and (dfTest is None)), \"testfile or dfTest at least one is set\"\n",
    "        assert not ((testfile is not None) and (dfTest is not None)), \"only one can be set\"\n",
    "\n",
    "        self.trainfile = trainfile\n",
    "        self.testfile = testfile\n",
    "        self.dfTrain = dfTrain\n",
    "        self.dfTest = dfTest\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.ignore_cols = ignore_cols\n",
    "        self.gen_feat_dict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def gen_feat_dict(self):\n",
    "        if self.dfTrain is None:\n",
    "            dfTrain = pd.read_csv(self.trainfile)\n",
    "\n",
    "        else:\n",
    "            dfTrain = self.dfTrain\n",
    "\n",
    "        if self.dfTest is None:\n",
    "            dfTest = pd.read_csv(self.testfile)\n",
    "\n",
    "        else:\n",
    "            dfTest = self.dfTest\n",
    "\n",
    "        df = pd.concat([dfTrain,dfTest])\n",
    "\n",
    "        self.feat_dict = {}\n",
    "        tc = 0\n",
    "        for col in df.columns:\n",
    "            if col in self.ignore_cols:\n",
    "                continue\n",
    "            if col in self.numeric_cols:\n",
    "                self.feat_dict[col] = tc\n",
    "                tc += 1\n",
    "\n",
    "            else:\n",
    "                us = df[col].unique()\n",
    "                print(us)\n",
    "                self.feat_dict[col] = dict(zip(us,range(tc,len(us)+tc)))\n",
    "                tc += len(us)\n",
    "\n",
    "        self.feat_dim = tc\n",
    "\n",
    "\n",
    "class DataParser(object):\n",
    "    def __init__(self,feat_dict):\n",
    "        self.feat_dict = feat_dict\n",
    "\n",
    "    def parse(self,infile=None,df=None,has_label=False):\n",
    "        assert not ((infile is None) and (df is None)), \"infile or df at least one is set\"\n",
    "        assert not ((infile is not None) and (df is not None)), \"only one can be set\"\n",
    "\n",
    "\n",
    "        if infile is None:\n",
    "            dfi = df.copy()\n",
    "        else:\n",
    "            dfi = pd.read_csv(infile)\n",
    "\n",
    "        if has_label:\n",
    "            y = dfi['target'].values.tolist()\n",
    "            dfi.drop(['id','target'],axis=1,inplace=True)\n",
    "        else:\n",
    "            ids = dfi['id'].values.tolist()\n",
    "            dfi.drop(['id'],axis=1,inplace=True)\n",
    "        # dfi for feature index\n",
    "        # dfv for feature value which can be either binary (1/0) or float (e.g., 10.24)\n",
    "        dfv = dfi.copy()\n",
    "        for col in dfi.columns:\n",
    "            if col in self.feat_dict.ignore_cols:\n",
    "                dfi.drop(col,axis=1,inplace=True)\n",
    "                dfv.drop(col,axis=1,inplace=True)\n",
    "                continue\n",
    "            if col in self.feat_dict.numeric_cols:\n",
    "                dfi[col] = self.feat_dict.feat_dict[col]\n",
    "            else:\n",
    "                dfi[col] = dfi[col].map(self.feat_dict.feat_dict[col])\n",
    "                dfv[col] = 1.\n",
    "\n",
    "        xi = dfi.values.tolist()\n",
    "        xv = dfv.values.tolist()\n",
    "\n",
    "        if has_label:\n",
    "            return xi,xv,y\n",
    "        else:\n",
    "            return xi,xv,ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\ipykernel_launcher.py:36: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 11  7  6  9  5  4  8  3  0  2  1 -1]\n",
      "[1 0]\n",
      "[-1  0  1]\n",
      "[0 1 8 9 2 6 3 7 4 5]\n",
      "[ 1 -1  0]\n",
      "[ 4 11 14 13  6 15  3  0  1 10 12  9 17  7  8  5  2 16]\n",
      "[ 1 -1  0]\n",
      "[0 1]\n",
      "[ 0  2  3  1 -1  4]\n",
      "[1 0 2]\n",
      "[2 3 1 0]\n",
      "[ 12  19  60 104  82  99  30  68  20  36 101 103  41  59  43  64  29  95\n",
      "  24   5  28  87  66  10  26  54  32  38  83  89  49  93   1  22  85  78\n",
      "  31  34   7   8   3  46  27  25  61  16  69  40  76  39  88  42  75  91\n",
      "  23   2  71  90  80  44  92  72  96  86  62  33  67  73  77  18  21  74\n",
      "  37  48  70  13  15 102  53  65 100  51  79  52  63  94   6  57  35  98\n",
      "  56  97  55  84  50   4  58   9  17  11  45  14  81  47]\n",
      "[2 1 5 0 4 3 6 7]\n",
      "[ 2  1  4  3 -1]\n",
      "[ 5  7  9  2  0  4  3  1 11  6  8 10]\n",
      "[ 1  0 -1]\n",
      "[ 0  1  4  3  6  5 -1  2]\n",
      "[0 1]\n",
      "[1 0]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1 2 3]\n",
      "[11  3 12  8  9  6 13  4 10  5  7  2  0  1]\n",
      "[0 1]\n",
      "[1 0]\n",
      "[0 1]\n",
      "id                         int64\n",
      "target                     int64\n",
      "ps_ind_01                  int64\n",
      "ps_ind_02_cat              int64\n",
      "ps_ind_03                  int64\n",
      "ps_ind_04_cat              int64\n",
      "ps_ind_05_cat              int64\n",
      "ps_ind_06_bin              int64\n",
      "ps_ind_07_bin              int64\n",
      "ps_ind_08_bin              int64\n",
      "ps_ind_09_bin              int64\n",
      "ps_ind_10_bin              int64\n",
      "ps_ind_11_bin              int64\n",
      "ps_ind_12_bin              int64\n",
      "ps_ind_13_bin              int64\n",
      "ps_ind_14                  int64\n",
      "ps_ind_15                  int64\n",
      "ps_ind_16_bin              int64\n",
      "ps_ind_17_bin              int64\n",
      "ps_ind_18_bin              int64\n",
      "ps_reg_01                float64\n",
      "ps_reg_02                float64\n",
      "ps_reg_03                float64\n",
      "ps_car_01_cat              int64\n",
      "ps_car_02_cat              int64\n",
      "ps_car_03_cat              int64\n",
      "ps_car_04_cat              int64\n",
      "ps_car_05_cat              int64\n",
      "ps_car_06_cat              int64\n",
      "ps_car_07_cat              int64\n",
      "                          ...   \n",
      "ps_car_09_cat              int64\n",
      "ps_car_10_cat              int64\n",
      "ps_car_11_cat              int64\n",
      "ps_car_11                  int64\n",
      "ps_car_12                float64\n",
      "ps_car_13                float64\n",
      "ps_car_14                float64\n",
      "ps_car_15                float64\n",
      "ps_calc_01               float64\n",
      "ps_calc_02               float64\n",
      "ps_calc_03               float64\n",
      "ps_calc_04                 int64\n",
      "ps_calc_05                 int64\n",
      "ps_calc_06                 int64\n",
      "ps_calc_07                 int64\n",
      "ps_calc_08                 int64\n",
      "ps_calc_09                 int64\n",
      "ps_calc_10                 int64\n",
      "ps_calc_11                 int64\n",
      "ps_calc_12                 int64\n",
      "ps_calc_13                 int64\n",
      "ps_calc_14                 int64\n",
      "ps_calc_15_bin             int64\n",
      "ps_calc_16_bin             int64\n",
      "ps_calc_17_bin             int64\n",
      "ps_calc_18_bin             int64\n",
      "ps_calc_19_bin             int64\n",
      "ps_calc_20_bin             int64\n",
      "missing_feat               int32\n",
      "ps_car_13_x_ps_reg_03    float64\n",
      "Length: 61, dtype: object\n",
      "#params: 23875\n",
      "epoch 0 loss [0.75041103]\n",
      "epoch 1 loss [0.680007]\n",
      "epoch 2 loss [0.52098686]\n",
      "epoch 3 loss [0.2887388]\n",
      "epoch 4 loss [0.16589002]\n",
      "epoch 5 loss [0.17680304]\n",
      "epoch 6 loss [0.1637809]\n",
      "epoch 7 loss [0.16503087]\n",
      "epoch 8 loss [0.1619512]\n",
      "epoch 9 loss [0.1590816]\n",
      "epoch 10 loss [0.15857214]\n",
      "epoch 11 loss [0.1578353]\n",
      "epoch 12 loss [0.15783203]\n",
      "epoch 13 loss [0.15787607]\n",
      "epoch 14 loss [0.15869652]\n",
      "epoch 15 loss [0.1586577]\n",
      "epoch 16 loss [0.15929917]\n",
      "epoch 17 loss [0.16004224]\n",
      "epoch 18 loss [0.16123368]\n",
      "epoch 19 loss [0.16239516]\n",
      "epoch 20 loss [0.16342627]\n",
      "epoch 21 loss [0.16514982]\n",
      "epoch 22 loss [0.16649759]\n",
      "epoch 23 loss [0.1677636]\n",
      "epoch 24 loss [0.17084178]\n",
      "epoch 25 loss [0.17078838]\n",
      "epoch 26 loss [0.17219698]\n",
      "epoch 27 loss [0.17408137]\n",
      "epoch 28 loss [0.17619891]\n",
      "epoch 29 loss [0.17960109]\n",
      "#params: 23875\n",
      "epoch 0 loss [0.5801402]\n",
      "epoch 1 loss [0.47886074]\n",
      "epoch 2 loss [0.30238673]\n",
      "epoch 3 loss [0.16652182]\n",
      "epoch 4 loss [0.19251554]\n",
      "epoch 5 loss [0.16942145]\n",
      "epoch 6 loss [0.16523862]\n",
      "epoch 7 loss [0.16967264]\n",
      "epoch 8 loss [0.16272119]\n",
      "epoch 9 loss [0.1616942]\n",
      "epoch 10 loss [0.16247942]\n",
      "epoch 11 loss [0.16506897]\n",
      "epoch 12 loss [0.16385494]\n",
      "epoch 13 loss [0.16234161]\n",
      "epoch 14 loss [0.16243228]\n",
      "epoch 15 loss [0.16326334]\n",
      "epoch 16 loss [0.1648886]\n",
      "epoch 17 loss [0.1643293]\n",
      "epoch 18 loss [0.16465583]\n",
      "epoch 19 loss [0.16520782]\n",
      "epoch 20 loss [0.16606952]\n",
      "epoch 21 loss [0.16644453]\n",
      "epoch 22 loss [0.16738744]\n",
      "epoch 23 loss [0.16828263]\n",
      "epoch 24 loss [0.16917983]\n",
      "epoch 25 loss [0.16992012]\n",
      "epoch 26 loss [0.17117332]\n",
      "epoch 27 loss [0.17276034]\n",
      "epoch 28 loss [0.1737511]\n",
      "epoch 29 loss [0.17470549]\n",
      "#params: 23875\n",
      "epoch 0 loss [0.69589335]\n",
      "epoch 1 loss [0.63934535]\n",
      "epoch 2 loss [0.502306]\n",
      "epoch 3 loss [0.27566904]\n",
      "epoch 4 loss [0.16930403]\n",
      "epoch 5 loss [0.16285636]\n",
      "epoch 6 loss [0.16141814]\n",
      "epoch 7 loss [0.16117536]\n",
      "epoch 8 loss [0.15969086]\n",
      "epoch 9 loss [0.16039439]\n",
      "epoch 10 loss [0.15916222]\n",
      "epoch 11 loss [0.15934715]\n",
      "epoch 12 loss [0.16072051]\n",
      "epoch 13 loss [0.16043527]\n",
      "epoch 14 loss [0.1614539]\n",
      "epoch 15 loss [0.16418631]\n",
      "epoch 16 loss [0.1631792]\n",
      "epoch 17 loss [0.1631993]\n",
      "epoch 18 loss [0.16440138]\n",
      "epoch 19 loss [0.16486052]\n",
      "epoch 20 loss [0.1647235]\n",
      "epoch 21 loss [0.16482985]\n",
      "epoch 22 loss [0.16723664]\n",
      "epoch 23 loss [0.16601738]\n",
      "epoch 24 loss [0.16719963]\n",
      "epoch 25 loss [0.1684955]\n",
      "epoch 26 loss [0.16837785]\n",
      "epoch 27 loss [0.17119282]\n",
      "epoch 28 loss [0.17194733]\n",
      "epoch 29 loss [0.17111523]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    dfTrain = pd.read_csv(TRAIN_FILE)\n",
    "    dfTest = pd.read_csv(TEST_FILE)\n",
    "\n",
    "    def preprocess(df):\n",
    "        cols = [c for c in df.columns if c not in ['id','target']]\n",
    "        #df['missing_feat'] = np.sum(df[df[cols]==-1].values,axis=1)\n",
    "        df[\"missing_feat\"] = np.sum((df[cols] == -1).values, axis=1)\n",
    "        df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "        return df\n",
    "\n",
    "    dfTrain = preprocess(dfTrain)\n",
    "    dfTest = preprocess(dfTest)\n",
    "\n",
    "    cols = [c for c in dfTrain.columns if c not in ['id','target']]\n",
    "    cols = [c for c in cols if (not c in IGNORE_COLS)]\n",
    "\n",
    "    X_train = dfTrain[cols].values\n",
    "    y_train = dfTrain['target'].values\n",
    "\n",
    "    X_test = dfTest[cols].values\n",
    "    ids_test = dfTest['id'].values\n",
    "\n",
    "    cat_features_indices = [i for i,c in enumerate(cols) if c in CATEGORICAL_COLS]\n",
    "\n",
    "    return dfTrain,dfTest,X_train,y_train,X_test,ids_test,cat_features_indices\n",
    "\n",
    "def run_base_model_pnn(dfTrain,dfTest,folds,pnn_params):\n",
    "    fd = FeatureDictionary(dfTrain=dfTrain,\n",
    "                           dfTest=dfTest,\n",
    "                           numeric_cols=NUMERIC_COLS,\n",
    "                           ignore_cols =IGNORE_COLS)\n",
    "    data_parser = DataParser(feat_dict= fd)\n",
    "    # Xi_train ：列的序号\n",
    "    # Xv_train ：列的对应的值\n",
    "    Xi_train,Xv_train,y_train = data_parser.parse(df=dfTrain,has_label=True)\n",
    "    Xi_test,Xv_test,ids_test = data_parser.parse(df=dfTest)\n",
    "\n",
    "    print(dfTrain.dtypes)\n",
    "\n",
    "    pnn_params['feature_size'] = fd.feat_dim\n",
    "    pnn_params['field_size'] = len(Xi_train[0])\n",
    "\n",
    "\n",
    "    _get = lambda x,l:[x[i] for i in l]\n",
    "\n",
    "\n",
    "\n",
    "    for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "        Xi_train_, Xv_train_, y_train_ = _get(Xi_train, train_idx), _get(Xv_train, train_idx), _get(y_train, train_idx)\n",
    "        Xi_valid_, Xv_valid_, y_valid_ = _get(Xi_train, valid_idx), _get(Xv_train, valid_idx), _get(y_train, valid_idx)\n",
    "\n",
    "        pnn = PNN(**pnn_params)\n",
    "        pnn.fit(Xi_train_, Xv_train_, y_train_, Xi_valid_, Xv_valid_, y_valid_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pnn_params = {\n",
    "    \"embedding_size\":8,\n",
    "    \"deep_layers\":[32,32],\n",
    "    \"dropout_deep\":[0.5,0.5,0.5],\n",
    "    \"deep_layer_activation\":tf.nn.relu,\n",
    "    \"epoch\":30,\n",
    "    \"batch_size\":1024,\n",
    "    \"learning_rate\":0.001,\n",
    "    \"optimizer\":\"adam\",\n",
    "    \"batch_norm\":1,\n",
    "    \"batch_norm_decay\":0.995,\n",
    "    \"verbose\":True,\n",
    "    \"random_seed\":RANDOM_SEED,\n",
    "    \"deep_init_size\":50,\n",
    "    \"use_inner\":False\n",
    "\n",
    "}\n",
    "\n",
    "# load data\n",
    "dfTrain, dfTest, X_train, y_train, X_test, ids_test, cat_features_indices = load_data()\n",
    "\n",
    "# folds\n",
    "folds = list(StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True,\n",
    "                             random_state=RANDOM_SEED).split(X_train, y_train))\n",
    "\n",
    "#y_train_pnn,y_test_pnn = run_base_model_pnn(dfTrain,dfTest,folds,pnn_params)\n",
    "run_base_model_pnn(dfTrain, dfTest, folds, pnn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
