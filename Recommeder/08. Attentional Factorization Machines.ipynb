{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "论文：[Attentional Factorization Machines:\n",
    "Learning the Weight of Feature Interactions via Attention Networks∗](http://www.nextcenter.org/wp-content/uploads/2018/02/Attentional-Factorization-Machines-Learning-the-Weight-of-Feature-Interactions-via-Attention-Networks.pdf)\n",
    "\n",
    "博客:[推荐系统遇上深度学习(八)--AFM模型理论和实践](https://blog.csdn.net/jiangjiang_jian/article/details/80674250)\n",
    "\n",
    "回顾FM:\n",
    "$$y= w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^{n-1} \\sum_{j = i+1}^n w_{ij}x_i x_j$$\n",
    "\n",
    "![image.png](pic/FM_formal.png)\n",
    "\n",
    "在进行预测时，FM会让一个特征固定一个特定的向量，当这个特征与其他特征做交叉时，都是用同样的向量去做计算。这个是很不合理的，因为不同的特征之间的交叉，重要程度是不一样的。如何体现这种重要程度，之前介绍的FFM模型是一个方案。另外，结合了attention机制的AFM模型，也是一种解决方案。\n",
    "\n",
    "关于什么是attention model？本文不打算详细赘述，我们这里只需要知道的是，attention机制相当于一个加权平均，attention的值就是其中权重，判断不同特征之间交互的重要性。\n",
    "\n",
    "刚才提到了，attention相等于加权的过程，因此我们的预测公式变为：\n",
    "$$y= w_0 + \\sum_{i=1}^n w_i x_i + p^T \\sum_{i=1}^{n-1} \\sum_{j = i+1}^n a_{ij}<v_i,v_j>x_i x_j$$\n",
    "<>含义是element-wise product，$p^T$是最后一层的权重。\n",
    "![image.png](pic/AFM1.png)\n",
    "图中的前三部分：sparse iput，embedding layer，pair-wise interaction layer(二次交叉特征)，都和FM是一样的。而后面的两部分，则是AFM的创新所在，也就是我们的Attention net。Attention背后的数学公式如下：\n",
    "$$a_{ij} = h^T Relu(W(<v_i,v_j>)x_ix_j+b)$$\n",
    "$$a_{ij} = \\frac{exp(a_{ij})}{\\sum_{i,j}exp(a_{ij})}$$\n",
    "\n",
    "总结一下，不难看出AFM只是在FM的基础上添加了attention的机制，但是实际上，由于最后的加权累加，二次项并没有进行更深的网络去学习非线性交叉特征，所以AFM并没有发挥出DNN的优势，也许结合DNN可以达到更好的结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class AFM(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, feature_size, field_size,\n",
    "                 embedding_size=8,attention_size=10,\n",
    "                 deep_layers=[32, 32], deep_init_size = 50,\n",
    "                 dropout_deep=[0.5, 0.5, 0.5],\n",
    "                 deep_layer_activation=tf.nn.relu,\n",
    "                 epoch=10, batch_size=256,\n",
    "                 learning_rate=0.001, optimizer=\"adam\",\n",
    "                 batch_norm=0, batch_norm_decay=0.995,\n",
    "                 verbose=False, random_seed=2016,\n",
    "                 loss_type=\"logloss\", eval_metric=roc_auc_score,\n",
    "                 greater_is_better=True,\n",
    "                 use_inner=True):\n",
    "        assert loss_type in [\"logloss\", \"mse\"], \\\n",
    "            \"loss_type can be either 'logloss' for classification task or 'mse' for regression task\"\n",
    "\n",
    "        self.feature_size = feature_size\n",
    "        self.field_size = field_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.attention_size = attention_size\n",
    "\n",
    "        self.deep_layers = deep_layers\n",
    "        self.deep_init_size = deep_init_size\n",
    "        self.dropout_dep = dropout_deep\n",
    "        self.deep_layers_activation = deep_layer_activation\n",
    "\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_decay = batch_norm_decay\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "        self.loss_type = loss_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.greater_is_better = greater_is_better\n",
    "        self.train_result,self.valid_result = [],[]\n",
    "\n",
    "        self.use_inner = use_inner\n",
    "\n",
    "        self._init_graph()\n",
    "\n",
    "    def _init_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "\n",
    "            self.feat_index = tf.placeholder(tf.int32,\n",
    "                                             shape=[None,None],\n",
    "                                             name='feat_index')\n",
    "            self.feat_value = tf.placeholder(tf.float32,\n",
    "                                             shape=[None,None],\n",
    "                                             name='feat_value')\n",
    "\n",
    "            self.label = tf.placeholder(tf.float32,shape=[None,1],name='label')\n",
    "            self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name='dropout_deep_deep')\n",
    "            self.train_phase = tf.placeholder(tf.bool,name='train_phase')\n",
    "\n",
    "            self.weights = self._initialize_weights()\n",
    "\n",
    "            # Embeddings\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'],self.feat_index) # N * F * K\n",
    "            feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])\n",
    "            self.embeddings = tf.multiply(self.embeddings,feat_value) # N * F * K\n",
    "\n",
    "\n",
    "            # element_wise\n",
    "            element_wise_product_list = []\n",
    "            for i in range(self.field_size):\n",
    "                for j in range(i+1,self.field_size):\n",
    "                    element_wise_product_list.append(tf.multiply(self.embeddings[:,i,:],self.embeddings[:,j,:])) # None * K\n",
    "\n",
    "            self.element_wise_product = tf.stack(element_wise_product_list) # (F * F - 1 / 2) * None * K\n",
    "            self.element_wise_product = tf.transpose(self.element_wise_product,perm=[1,0,2],name='element_wise_product') # None * (F * F - 1 / 2) *  K\n",
    "\n",
    "            #self.interaction\n",
    "\n",
    "            # attention part\n",
    "            num_interactions = int(self.field_size * (self.field_size - 1) / 2)\n",
    "            # wx+b -> relu(wx+b) -> h*relu(wx+b)\n",
    "            self.attention_wx_plus_b = tf.reshape(tf.add(tf.matmul(tf.reshape(self.element_wise_product,shape=(-1,self.embedding_size)),\n",
    "                                                                   self.weights['attention_w']),\n",
    "                                                         self.weights['attention_b']),\n",
    "                                                  shape=[-1,num_interactions,self.attention_size]) # N * ( F * F - 1 / 2) * A\n",
    "\n",
    "            self.attention_exp = tf.exp(tf.reduce_sum(tf.multiply(tf.nn.relu(self.attention_wx_plus_b),\n",
    "                                                                  self.weights['attention_h']),\n",
    "                                                      axis=2,keep_dims=True)) # N * ( F * F - 1 / 2) * 1\n",
    "\n",
    "            self.attention_exp_sum = tf.reduce_sum(self.attention_exp,axis=1,keep_dims=True) # N * 1 * 1\n",
    "\n",
    "            self.attention_out = tf.div(self.attention_exp,self.attention_exp_sum,name='attention_out')  # N * ( F * F - 1 / 2) * 1\n",
    "\n",
    "            self.attention_x_product = tf.reduce_sum(tf.multiply(self.attention_out,self.element_wise_product),axis=1,name='afm') # N * K\n",
    "\n",
    "            self.attention_part_sum = tf.matmul(self.attention_x_product,self.weights['attention_p']) # N * 1\n",
    "\n",
    "\n",
    "\n",
    "            # first order term\n",
    "            self.y_first_order = tf.nn.embedding_lookup(self.weights['feature_bias'], self.feat_index)\n",
    "            self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order, feat_value), 2)\n",
    "\n",
    "            # bias\n",
    "            self.y_bias = self.weights['bias'] * tf.ones_like(self.label)\n",
    "\n",
    "\n",
    "            # out\n",
    "            self.out = tf.add_n([tf.reduce_sum(self.y_first_order,axis=1,keep_dims=True),\n",
    "                                 self.attention_part_sum,\n",
    "                                 self.y_bias],name='out_afm')\n",
    "\n",
    "            # loss\n",
    "            if self.loss_type == \"logloss\":\n",
    "                self.out = tf.nn.sigmoid(self.out)\n",
    "                self.loss = tf.losses.log_loss(self.label, self.out)\n",
    "            elif self.loss_type == \"mse\":\n",
    "                self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))\n",
    "\n",
    "\n",
    "\n",
    "            if self.optimizer_type == \"adam\":\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,\n",
    "                                                        epsilon=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"adagrad\":\n",
    "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n",
    "                                                           initial_accumulator_value=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"gd\":\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"momentum\":\n",
    "                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize(\n",
    "                    self.loss)\n",
    "\n",
    "\n",
    "            #init\n",
    "            self.saver = tf.train.Saver()\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(init)\n",
    "\n",
    "            # number of params\n",
    "            total_parameters = 0\n",
    "            for variable in self.weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            if self.verbose > 0:\n",
    "                print(\"#params: %d\" % total_parameters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        weights = dict()\n",
    "\n",
    "        #embeddings\n",
    "        weights['feature_embeddings'] = tf.Variable(\n",
    "            tf.random_normal([self.feature_size,self.embedding_size],0.0,0.01),\n",
    "            name='feature_embeddings')\n",
    "        weights['feature_bias'] = tf.Variable(tf.random_normal([self.feature_size,1],0.0,1.0),name='feature_bias')\n",
    "        weights['bias'] = tf.Variable(tf.constant(0.1),name='bias')\n",
    "\n",
    "        # attention part\n",
    "        glorot = np.sqrt(2.0 / (self.attention_size + self.embedding_size))\n",
    "\n",
    "        weights['attention_w'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(self.embedding_size,self.attention_size)),\n",
    "                                             dtype=tf.float32,name='attention_w')\n",
    "\n",
    "        weights['attention_b'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(self.attention_size,)),\n",
    "                                             dtype=tf.float32,name='attention_b')\n",
    "\n",
    "        weights['attention_h'] = tf.Variable(np.random.normal(loc=0,scale=1,size=(self.attention_size,)),\n",
    "                                             dtype=tf.float32,name='attention_h')\n",
    "\n",
    "\n",
    "        weights['attention_p'] = tf.Variable(np.ones((self.embedding_size,1)),dtype=np.float32)\n",
    "\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def get_batch(self,Xi,Xv,y,batch_size,index):\n",
    "        start = index * batch_size\n",
    "        end = (index + 1) * batch_size\n",
    "        end = end if end < len(y) else len(y)\n",
    "        return Xi[start:end],Xv[start:end],[[y_] for y_ in y[start:end]]\n",
    "\n",
    "    # shuffle three lists simutaneously\n",
    "    def shuffle_in_unison_scary(self, a, b, c):\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "\n",
    "    def predict(self, Xi, Xv,y):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :return: predicted probability of each sample\n",
    "        \"\"\"\n",
    "        # dummy y\n",
    "        feed_dict = {self.feat_index: Xi,\n",
    "                     self.feat_value: Xv,\n",
    "                     self.label: y,\n",
    "                     self.dropout_keep_deep: [1.0] * len(self.dropout_dep),\n",
    "                     self.train_phase: True}\n",
    "\n",
    "        loss = self.sess.run([self.loss], feed_dict=feed_dict)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def fit_on_batch(self,Xi,Xv,y):\n",
    "        feed_dict = {self.feat_index:Xi,\n",
    "                     self.feat_value:Xv,\n",
    "                     self.label:y,\n",
    "                     self.dropout_keep_deep:self.dropout_dep,\n",
    "                     self.train_phase:True}\n",
    "\n",
    "        loss,opt = self.sess.run([self.loss,self.optimizer],feed_dict=feed_dict)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fit(self, Xi_train, Xv_train, y_train,\n",
    "            Xi_valid=None, Xv_valid=None, y_valid=None,\n",
    "            early_stopping=False, refit=False):\n",
    "\n",
    "        has_valid = Xv_valid is not None\n",
    "        for epoch in range(self.epoch):\n",
    "            t1 = time()\n",
    "            self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "            total_batch = int(len(y_train) / self.batch_size)\n",
    "            for i in range(total_batch):\n",
    "                Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train, self.batch_size, i)\n",
    "                self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "\n",
    "            if has_valid:\n",
    "                y_valid = np.array(y_valid).reshape((-1,1))\n",
    "                loss = self.predict(Xi_valid, Xv_valid, y_valid)\n",
    "                print(\"epoch\",epoch,\"loss\",loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
