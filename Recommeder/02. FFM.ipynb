{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field-aware Factorization Machine\n",
    "\n",
    "参考：\n",
    "[推荐系统遇上深度学习(二)--FFM模型理论和实践](https://blog.csdn.net/jiangjiang_jian/article/details/80630903)\n",
    "\n",
    "与FM中每一维特征只由一个隐含特征向量表示不同，FFM中每一个特征在于不同field的特征进行组合时，所使用的隐含特征向量不同。因此，有ＦＭ的推导公式：\n",
    "$$y(x) = w_0 +　\\sum_{i=1}^n w_ix_i + \\sum_{i=1}^n\\sum_{j=i+1}^n<V_i,V_j>x_ix_j$$ \n",
    "可以推导出FFM的推导公式：\n",
    "$$y(x) = w_0 +　\\sum_{i=1}^n w_ix_i + \\sum_{i=1}^n\\sum_{j=i+1}^n<V_{i,f_j},V_{j,f_i}>x_ix_j$$ \n",
    "可以看到，如果隐向量的长度为 k，那么FFM的二次参数有 nfk 个，远多于FM模型的 nk个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 O(kn^2)。\n",
    "\n",
    "这里讲FFM的一种实现细节，把FFM看做{-1,1}的分类问题，也可以将FFM看做和FM相同的回归问题\n",
    "\n",
    "使用logistic损失函数:\n",
    "$$min_w \\sum_{i=1}^L log(1+exp\\{-y_i \\phi(w,x_i)\\})+ \\frac{\\lambda}{2}||w||^2$$\n",
    "\n",
    "# 生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-879d9bebeb9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mdata_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"feature num {} field num {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mffm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFFM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prepare_data' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "class FFM:\n",
    "    def __init__(self, batch_size,learning_rate, data_path, field_num, feature_num,feature2field,data_set):\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.data_path = data.path\n",
    "        self.field_num = field_num\n",
    "        self.feature_num = feature_num\n",
    "        self.feature2field = feature2field\n",
    "        self.data_set = data_set\n",
    "        \n",
    "        with tf.name_scope('embedding_matrix'):\n",
    "            self.linear_weight = tf.get_variable(name = 'linear_weight',\n",
    "                                         shape=[feature_num],\n",
    "                                         dtype = tf.float32,\n",
    "                                         initializer=tf.truncated_normal_initializer(stddev = 0.01))\n",
    "            \n",
    "            tf.summary.histogram('linear_weight',self.linear_weight)\n",
    "            self.field_embedding=[]\n",
    "            for idx in xrange(0,self.feature_num):\n",
    "                self.field_embedding.append(tf.get_variable(name='field_embedding{}'.format(idx),\n",
    "                                                            shape = [field_num],\n",
    "                                                            dtype = tf.float32,\n",
    "                                                            initializer = tf.truncated_normal_initializer(stddev = 0.01)\n",
    "                                                           ))\n",
    "                tf.summary.histogram('field_vector{}'%format(idx),self.field_embedding[idx])\n",
    "            \n",
    "        \n",
    "        with tf.name_scope('input'):\n",
    "            self.label = tf.placeholdera(tf.float32,shape = (self.batch_size))\n",
    "            \n",
    "            self.feature_value = []\n",
    "            \n",
    "            for idx in xrange(0,feature_num):\n",
    "                self.feature_value.append(tf.placeholder(tf.float32,\n",
    "                                                        shape = (self.batch_size),\n",
    "                                                        name = 'feature_{}'%format(idx)))\n",
    "                \n",
    "                \n",
    "                \n",
    "        with tf.name_scope('network'):\n",
    "            \n",
    "            # b0:constant bias\n",
    "            # predict = b0 + sum(Vi * feature_i) + sum(Vij * Vji * feature_i * feature_j)\n",
    "            self.b0 = tf.get_variable(name = 'bias_0', shape=[1], dtype = tf.float32)\n",
    "            \n",
    "            tf.summary.histogram('bo',self.b0)\n",
    "            \n",
    "            self.linear_term = tf.reduce_sum(tf.multiply(tf.transpose(tf.convert_to_tensor(self.feature_value),perm=[1,0]),\n",
    "                                                         self.linear_weight))\n",
    "            \n",
    "            self.qua_term = tf.get_variable(name = 'quad_term',shape=[1],dtype= tf.float)\n",
    "            \n",
    "            for f1 in xrange(0,feature_num-1):\n",
    "                for f2 in xrange(f1+1,feature_num):\n",
    "                    # 从f1特征的embedding里，找f2的 feature embedding\n",
    "                    w1 = tf.nn.embedding_lookup(self.field_embedding[f1],self.feature2field[f2])\n",
    "                    w2 = tf.nn.embedding_lookup(self.field_embedding[f2],self.feature2field[f1])\n",
    "                    self.qua_term += w1*w2*self.feature_value[f1]*self.feature_value[f2]\n",
    "            \n",
    "            self.predict = self.b0 + self.linear_term + self.qua_term\n",
    "            self.losses = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels = self.labels,logits = self.predict))\n",
    "            tf.summary.scalar('losses',self.losses)\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate = self.lr,name ='Adam')\n",
    "            \n",
    "            self.grad = self.optimizer.compute_gradient(self.losses)\n",
    "            \n",
    "            self.opt = self.optimizer.apply_gradients(self.grad)\n",
    "            \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        \n",
    "        with tf.name_scope('plot'):\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            self.writer = tf.summary.FileWriter('./train_plot', self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.loop_step = 0\n",
    "        \n",
    "        \n",
    "    def step(self):\n",
    "        '''\n",
    "        return :log_loss\n",
    "        '''\n",
    "        self.loop_step += 1\n",
    "        feature, label =  self.get_data()\n",
    "        feed_dict = {}\n",
    "        feed_dict[self.label] = label\n",
    "        arr_feature = np.transpose(np.array(feature))\n",
    "        for idx in xrange(0,self.feature_num):\n",
    "            feed_dict[self.feature_value[idx]] = arr_feature[idx]\n",
    "        _,summary,loss_value = self.sess.run([self.opt,self.merged,self.losses],feed_dict=feed_dict)\n",
    "        self.writer.add_summary(summary, self.loop_step)\n",
    "        return loss_value\n",
    "        \n",
    "    \n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        :return: a tuple of feature and label\n",
    "        feature: shape[batch_size ,feature_num] each element is a sclar\n",
    "        label:[batch_size] each element is 0 or 1\n",
    "        \"\"\"\n",
    "        feature = []\n",
    "        label = []\n",
    "        for _ in xrange(0, self.batch_size):\n",
    "            t_feature = [0.0] * feature_num\n",
    "            sample = self.data_set[random.randint(0, len(self.data_set) - 1)]\n",
    "            label.append(sample[-1])\n",
    "            sample = sample[:-1]\n",
    "            for f in sample:\n",
    "                t_feature[int(f.split(':')[0])] = float(f.split(':')[1])\n",
    "            feature.append(t_feature)\n",
    "        return feature, label\n",
    "        \n",
    "                                          \n",
    "if __name__ == \"__main__\":\n",
    "    data_set, feature_map = prepare_data(file_path=data_path)\n",
    "    print(\"feature num {} field num {}\".format(feature_num, field_num))\n",
    "    ffm = FFM(batch_size, learning_rate, data_path, field_num, feature_num, feature_map, data_set)\n",
    "    feature, label = ffm.get_data()\n",
    "    for loop in xrange(0, 1000):\n",
    "        losses = ffm.step()\n",
    "        if (loop % 50):\n",
    "            print(\"loop:{} losses:{}\".format(loop, losses))                \n",
    "            \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
