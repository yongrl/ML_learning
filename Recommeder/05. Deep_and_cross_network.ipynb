{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "博客：[推荐系统遇上深度学习(五)--Deep&amp;Cross Network模型理论和实践](https://blog.csdn.net/jiangjiang_jian/article/details/80674084)\n",
    "\n",
    "论文：[Deep & Cross Network for Ad Click Predictions,2017](https://arxiv.org/pdf/1708.05123.pdf)\n",
    "Deep&Cross Network模型我们下面将简称DCN模型：\n",
    "\n",
    "一个DCN模型从嵌入和堆积层开始，接着是一个交叉网络和一个与之平行的深度网络，之后是最后的组合层，它结合了两个网络的输出。完整的网络模型如图：\n",
    "![image.png](pic/DNC1.png)\n",
    "\n",
    "# 嵌入和堆叠层\n",
    "我们考虑具有离散和连续特征的输入数据。在网络规模推荐系统中，如CTR预测，输入主要是分类特征，如“country=usa”。这些特征通常是编码为独热向量如“[ 0,1,0 ]”；然而，这往往导致过度的高维特征空间大的词汇。\n",
    "\n",
    "为了减少维数，我们采用嵌入过程将这些离散特征转换成实数值的稠密向量（通常称为嵌入向量）,然后，我们将嵌入向量与连续特征向量叠加起来形成一个向量,拼接起来的向量X0将作为我们Cross Network和Deep Network的输入。\n",
    "\n",
    "# Cross NetWork\n",
    "交叉网络的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式：\n",
    "$$x_{l+1} = x_0x_{l}^Tw_l +b_l+x_l = f(x_l,w_l,b_l)+x_l$$\n",
    "一个交叉层的可视化如图所示:\n",
    "![image.png](pic/DNC2.png)\n",
    "cross network相当于学习了n（cross层数）次cross_feature特征，且使用了贪心策略，每一步的网络交叉函数f只学习了l次多项式特征和l-1 次多项式特征的残差$x_{l}-x_{l-1}$\n",
    "\n",
    "cross network，其主优势：\n",
    "\n",
    "- 在多个层都实现了自动化的cross feature，也不需要进行人工特征工程\n",
    "- 最高的多项式度在每一层增加，并由层深度决定。 网络由度数的所有交叉项组成，直到最高，其系数都不同。\n",
    "- cross network内存效率高，并且利于实现\n",
    "- 本文的实验结果显示，DCN的logloss小于DNN，且其参数数量几乎少一个数量级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class DCN(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cate_feature_size, field_size,numeric_feature_size,\n",
    "                 embedding_size=8,\n",
    "                 deep_layers=[32, 32], dropout_deep=[0.5, 0.5, 0.5],\n",
    "                 deep_layers_activation=tf.nn.relu,\n",
    "                 epoch=10, batch_size=256,\n",
    "                 learning_rate=0.001, optimizer_type=\"adam\",\n",
    "                 batch_norm=0, batch_norm_decay=0.995,\n",
    "                 verbose=False, random_seed=2016,\n",
    "                 loss_type=\"logloss\", eval_metric=roc_auc_score,\n",
    "                 l2_reg=0.0, greater_is_better=True,cross_layer_num=3):\n",
    "        assert loss_type in [\"logloss\", \"mse\"], \\\n",
    "            \"loss_type can be either 'logloss' for classification task or 'mse' for regression task\"\n",
    "\n",
    "        self.cate_feature_size = cate_feature_size\n",
    "        self.numeric_feature_size = numeric_feature_size\n",
    "        self.field_size = field_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.total_size = self.field_size * self.embedding_size + self.numeric_feature_size\n",
    "        self.deep_layers = deep_layers\n",
    "        self.cross_layer_num = cross_layer_num\n",
    "        self.dropout_dep = dropout_deep\n",
    "        self.deep_layers_activation = deep_layers_activation\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer_type\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_decay = batch_norm_decay\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "        self.loss_type = loss_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.greater_is_better = greater_is_better\n",
    "        self.train_result,self.valid_result = [],[]\n",
    "\n",
    "        self._init_graph()\n",
    "\n",
    "    def _init_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "\n",
    "            self.feat_index = tf.placeholder(tf.int32,\n",
    "                                             shape=[None,None],\n",
    "                                             name='feat_index')\n",
    "            self.feat_value = tf.placeholder(tf.float32,\n",
    "                                           shape=[None,None],\n",
    "                                           name='feat_value')\n",
    "\n",
    "            self.numeric_value = tf.placeholder(tf.float32,[None,None],name='num_value')\n",
    "\n",
    "            self.label = tf.placeholder(tf.float32,shape=[None,1],name='label')\n",
    "            self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name='dropout_deep_deep')\n",
    "            self.train_phase = tf.placeholder(tf.bool,name='train_phase')\n",
    "\n",
    "            self.weights = self._initialize_weights()\n",
    "\n",
    "            # model\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'],self.feat_index) # N * F * K\n",
    "            feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])\n",
    "            self.embeddings = tf.multiply(self.embeddings,feat_value)\n",
    "\n",
    "            self.x0 = tf.concat([self.numeric_value,\n",
    "                                 tf.reshape(self.embeddings,shape=[-1,self.field_size * self.embedding_size])]\n",
    "                                ,axis=1)\n",
    "\n",
    "\n",
    "            # deep part\n",
    "\n",
    "\n",
    "            self.y_deep = tf.nn.dropout(self.x0,self.dropout_keep_deep[0])\n",
    "\n",
    "            for i in range(0,len(self.deep_layers)):\n",
    "                self.y_deep = tf.add(tf.matmul(self.y_deep,self.weights[\"deep_layer_%d\" %i]), self.weights[\"deep_bias_%d\"%i])\n",
    "                self.y_deep = self.deep_layers_activation(self.y_deep)\n",
    "                self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[i+1])\n",
    "\n",
    "\n",
    "            # cross_part\n",
    "            self._x0 = tf.reshape(self.x0, (-1, self.total_size, 1))\n",
    "            x_l = self._x0\n",
    "            for l in range(self.cross_layer_num):\n",
    "                x_l = tf.tensordot(tf.matmul(self._x0, x_l, transpose_b=True),\n",
    "                                    self.weights[\"cross_layer_%d\" % l],1) + self.weights[\"cross_bias_%d\" % l] + x_l\n",
    "\n",
    "            self.cross_network_out = tf.reshape(x_l, (-1, self.total_size))\n",
    "\n",
    "\n",
    "            # concat_part\n",
    "            concat_input = tf.concat([self.cross_network_out, self.y_deep], axis=1)\n",
    "\n",
    "            self.out = tf.add(tf.matmul(concat_input,self.weights['concat_projection']),self.weights['concat_bias'])\n",
    "\n",
    "            # loss\n",
    "            if self.loss_type == \"logloss\":\n",
    "                self.out = tf.nn.sigmoid(self.out)\n",
    "                self.loss = tf.losses.log_loss(self.label, self.out)\n",
    "            elif self.loss_type == \"mse\":\n",
    "                self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))\n",
    "            # l2 regularization on weights\n",
    "            if self.l2_reg > 0:\n",
    "                self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                    self.l2_reg)(self.weights[\"concat_projection\"])\n",
    "                for i in range(len(self.deep_layers)):\n",
    "                    self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                        self.l2_reg)(self.weights[\"deep_layer_%d\" % i])\n",
    "                for i in range(self.cross_layer_num):\n",
    "                    self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                        self.l2_reg)(self.weights[\"cross_layer_%d\" % i])\n",
    "\n",
    "\n",
    "            if self.optimizer_type == \"adam\":\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,\n",
    "                                                        epsilon=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"adagrad\":\n",
    "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n",
    "                                                           initial_accumulator_value=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"gd\":\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"momentum\":\n",
    "                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize(\n",
    "                    self.loss)\n",
    "\n",
    "\n",
    "            #init\n",
    "            self.saver = tf.train.Saver()\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(init)\n",
    "\n",
    "            # number of params\n",
    "            total_parameters = 0\n",
    "            for variable in self.weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            if self.verbose > 0:\n",
    "                print(\"#params: %d\" % total_parameters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        weights = dict()\n",
    "\n",
    "        #embeddings\n",
    "        weights['feature_embeddings'] = tf.Variable(\n",
    "            tf.random_normal([self.cate_feature_size,self.embedding_size],0.0,0.01),\n",
    "            name='feature_embeddings')\n",
    "        weights['feature_bias'] = tf.Variable(tf.random_normal([self.cate_feature_size,1],0.0,1.0),name='feature_bias')\n",
    "\n",
    "\n",
    "        #deep layers\n",
    "        num_layer = len(self.deep_layers)\n",
    "        glorot = np.sqrt(2.0/(self.total_size + self.deep_layers[0]))\n",
    "\n",
    "        weights['deep_layer_0'] = tf.Variable(\n",
    "            np.random.normal(loc=0,scale=glorot,size=(self.total_size,self.deep_layers[0])),dtype=np.float32\n",
    "        )\n",
    "        weights['deep_bias_0'] = tf.Variable(\n",
    "            np.random.normal(loc=0,scale=glorot,size=(1,self.deep_layers[0])),dtype=np.float32\n",
    "        )\n",
    "\n",
    "\n",
    "        for i in range(1,num_layer):\n",
    "            glorot = np.sqrt(2.0 / (self.deep_layers[i - 1] + self.deep_layers[i]))\n",
    "            weights[\"deep_layer_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i - 1], self.deep_layers[i])),\n",
    "                dtype=np.float32)  # layers[i-1] * layers[i]\n",
    "            weights[\"deep_bias_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),\n",
    "                dtype=np.float32)  # 1 * layer[i]\n",
    "\n",
    "        for i in range(self.cross_layer_num):\n",
    "\n",
    "            weights[\"cross_layer_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(self.total_size,1)),\n",
    "                dtype=np.float32)\n",
    "            weights[\"cross_bias_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(self.total_size,1)),\n",
    "                dtype=np.float32)  # 1 * layer[i]\n",
    "\n",
    "        # final concat projection layer\n",
    "\n",
    "\n",
    "        input_size = self.total_size + self.deep_layers[-1]\n",
    "\n",
    "        glorot = np.sqrt(2.0/(input_size + 1))\n",
    "        weights['concat_projection'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(input_size,1)),dtype=np.float32)\n",
    "        weights['concat_bias'] = tf.Variable(tf.constant(0.01),dtype=np.float32)\n",
    "\n",
    "\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def get_batch(self,Xi,Xv,Xv2,y,batch_size,index):\n",
    "        start = index * batch_size\n",
    "        end = (index + 1) * batch_size\n",
    "        end = end if end < len(y) else len(y)\n",
    "        return Xi[start:end],Xv[start:end],Xv2[start:end],[[y_] for y_ in y[start:end]]\n",
    "\n",
    "    # shuffle three lists simutaneously\n",
    "    def shuffle_in_unison_scary(self, a, b, c,d):\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(d)\n",
    "\n",
    "    def predict(self, Xi, Xv,Xv2,y):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :return: predicted probability of each sample\n",
    "        \"\"\"\n",
    "        # dummy y\n",
    "\n",
    "        feed_dict = {self.feat_index: Xi,\n",
    "                     self.feat_value: Xv,\n",
    "                     self.numeric_value: Xv2,\n",
    "                     self.label: y,\n",
    "                     self.dropout_keep_deep: [1.0] * len(self.dropout_dep),\n",
    "                     self.train_phase: True}\n",
    "\n",
    "        loss = self.sess.run([self.loss], feed_dict=feed_dict)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def fit_on_batch(self,Xi,Xv,Xv2,y):\n",
    "        feed_dict = {self.feat_index:Xi,\n",
    "                     self.feat_value:Xv,\n",
    "                     self.numeric_value:Xv2,\n",
    "                     self.label:y,\n",
    "                     self.dropout_keep_deep:self.dropout_dep,\n",
    "                     self.train_phase:True}\n",
    "\n",
    "        loss,opt = self.sess.run([self.loss,self.optimizer],feed_dict=feed_dict)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fit(self, cate_Xi_train, cate_Xv_train,numeric_Xv_train, y_train,\n",
    "            cate_Xi_valid=None, cate_Xv_valid=None, numeric_Xv_valid=None,y_valid=None,\n",
    "            early_stopping=False, refit=False):\n",
    "        \"\"\"\n",
    "        :param Xi_train: [[ind1_1, ind1_2, ...], [ind2_1, ind2_2, ...], ..., [indi_1, indi_2, ..., indi_j, ...], ...]\n",
    "                         indi_j is the feature index of feature field j of sample i in the training set\n",
    "        :param Xv_train: [[val1_1, val1_2, ...], [val2_1, val2_2, ...], ..., [vali_1, vali_2, ..., vali_j, ...], ...]\n",
    "                         vali_j is the feature value of feature field j of sample i in the training set\n",
    "                         vali_j can be either binary (1/0, for binary/categorical features) or float (e.g., 10.24, for numerical features)\n",
    "        :param y_train: label of each sample in the training set\n",
    "        :param Xi_valid: list of list of feature indices of each sample in the validation set\n",
    "        :param Xv_valid: list of list of feature values of each sample in the validation set\n",
    "        :param y_valid: label of each sample in the validation set\n",
    "        :param early_stopping: perform early stopping or not\n",
    "        :param refit: refit the model on the train+valid dataset or not\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(len(cate_Xi_train))\n",
    "        print(len(cate_Xv_train))\n",
    "        print(len(numeric_Xv_train))\n",
    "        print(len(y_train))\n",
    "        has_valid = cate_Xv_valid is not None\n",
    "        for epoch in range(self.epoch):\n",
    "            t1 = time()\n",
    "            self.shuffle_in_unison_scary(cate_Xi_train, cate_Xv_train,numeric_Xv_train, y_train)\n",
    "            total_batch = int(len(y_train) / self.batch_size)\n",
    "            for i in range(total_batch):\n",
    "                cate_Xi_batch, cate_Xv_batch,numeric_Xv_batch, y_batch = self.get_batch(cate_Xi_train, cate_Xv_train, numeric_Xv_train,y_train, self.batch_size, i)\n",
    "\n",
    "                self.fit_on_batch(cate_Xi_batch, cate_Xv_batch,numeric_Xv_batch, y_batch)\n",
    "\n",
    "\n",
    "            if has_valid:\n",
    "                y_valid = np.array(y_valid).reshape((-1,1))\n",
    "                loss = self.predict(cate_Xi_valid, cate_Xv_valid, numeric_Xv_valid, y_valid)\n",
    "                print(\"epoch\",epoch,\"loss\",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"data/DCN_train.csv\"\n",
    "TEST_FILE = \"data/DCN_test.csv\"\n",
    "\n",
    "SUB_DIR = \"output\"\n",
    "\n",
    "\n",
    "NUM_SPLITS = 3\n",
    "RANDOM_SEED = 2017\n",
    "\n",
    "# types of columns of the dataset dataframe\n",
    "CATEGORICAL_COLS = [\n",
    "    'ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat',\n",
    "    'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat',\n",
    "    'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat',\n",
    "    'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat',\n",
    "    'ps_car_10_cat', 'ps_car_11_cat',\n",
    "]\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    # # binary\n",
    "    # \"ps_ind_06_bin\", \"ps_ind_07_bin\", \"ps_ind_08_bin\",\n",
    "    # \"ps_ind_09_bin\", \"ps_ind_10_bin\", \"ps_ind_11_bin\",\n",
    "    # \"ps_ind_12_bin\", \"ps_ind_13_bin\", \"ps_ind_16_bin\",\n",
    "    # \"ps_ind_17_bin\", \"ps_ind_18_bin\",\n",
    "    # \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\",\n",
    "    # \"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\",\n",
    "    # numeric\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\",\n",
    "    \"ps_car_12\", \"ps_car_13\", \"ps_car_14\", \"ps_car_15\",\n",
    "\n",
    "    # feature engineering\n",
    "    \"missing_feat\", \"ps_car_13_x_ps_reg_03\",\n",
    "]\n",
    "\n",
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\",\n",
    "    \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\",\n",
    "    \"ps_calc_09\", \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\",\n",
    "    \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\",\n",
    "    \"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class FeatureDictionary(object):\n",
    "    def __init__(self, trainfile=None,testfile=None,\n",
    "                 numeric_cols=[],\n",
    "                 ignore_cols=[],\n",
    "                 cate_cols=[]):\n",
    "\n",
    "        self.trainfile = trainfile\n",
    "        #self.testfile = testfile\n",
    "        self.testfile = testfile\n",
    "        self.cate_cols = cate_cols\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.ignore_cols = ignore_cols\n",
    "        self.gen_feat_dict()\n",
    "\n",
    "    def gen_feat_dict(self):\n",
    "        df = pd.concat([self.trainfile,self.testfile])\n",
    "        self.feat_dict = {}\n",
    "        self.feat_len = {}\n",
    "        tc = 0\n",
    "        for col in df.columns:\n",
    "            if col in self.ignore_cols or col in self.numeric_cols:\n",
    "                continue\n",
    "            else:\n",
    "                us = df[col].unique()\n",
    "                self.feat_dict[col] = dict(zip(us, range(tc, len(us) + tc)))\n",
    "                tc += len(us)\n",
    "        self.feat_dim = tc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataParser(object):\n",
    "    def __init__(self, feat_dict):\n",
    "        self.feat_dict = feat_dict\n",
    "\n",
    "\n",
    "    def parse(self, infile=None, df=None, has_label=False):\n",
    "        assert not ((infile is None) and (df is None)), \"infile or df at least one is set\"\n",
    "        assert not ((infile is not None) and (df is not None)), \"only one can be set\"\n",
    "        if infile is None:\n",
    "            dfi = df.copy()\n",
    "        else:\n",
    "            dfi = pd.read_csv(infile)\n",
    "        if has_label:\n",
    "            y = dfi[\"target\"].values.tolist()\n",
    "            dfi.drop([\"id\", \"target\"], axis=1, inplace=True)\n",
    "        else:\n",
    "            ids = dfi[\"id\"].values.tolist()\n",
    "            dfi.drop([\"id\"], axis=1, inplace=True)\n",
    "        # dfi for feature index\n",
    "        # dfv for feature value which can be either binary (1/0) or float (e.g., 10.24)\n",
    "\n",
    "        numeric_Xv = dfi[self.feat_dict.numeric_cols].values.tolist()\n",
    "        dfi.drop(self.feat_dict.numeric_cols,axis=1,inplace=True)\n",
    "\n",
    "        dfv = dfi.copy()\n",
    "        for col in dfi.columns:\n",
    "            if col in self.feat_dict.ignore_cols:\n",
    "                dfi.drop(col, axis=1, inplace=True)\n",
    "                dfv.drop(col, axis=1, inplace=True)\n",
    "                continue\n",
    "            else:\n",
    "                dfi[col] = dfi[col].map(self.feat_dict.feat_dict[col])\n",
    "                dfv[col] = 1.\n",
    "\n",
    "        # list of list of feature indices of each sample in the dataset\n",
    "        cate_Xi = dfi.values.tolist()\n",
    "        # list of list of feature values of each sample in the dataset\n",
    "        cate_Xv = dfv.values.tolist()\n",
    "        if has_label:\n",
    "            return cate_Xi, cate_Xv,numeric_Xv,y\n",
    "        else:\n",
    "            return cate_Xi, cate_Xv,numeric_Xv,ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data_over\n",
      "process_data_over\n",
      "start train\n",
      "247\n",
      "{'ps_car_01_cat': {10: 0, 11: 1, 7: 2, 6: 3, 9: 4, 5: 5, 4: 6, 8: 7, 3: 8, 0: 9, 2: 10, 1: 11, -1: 12}, 'ps_car_02_cat': {1: 13, 0: 14}, 'ps_car_03_cat': {-1: 15, 0: 16, 1: 17}, 'ps_car_04_cat': {0: 18, 1: 19, 8: 20, 9: 21, 2: 22, 6: 23, 3: 24, 7: 25, 4: 26, 5: 27}, 'ps_car_05_cat': {1: 28, -1: 29, 0: 30}, 'ps_car_06_cat': {4: 31, 11: 32, 14: 33, 13: 34, 6: 35, 15: 36, 3: 37, 0: 38, 1: 39, 10: 40, 12: 41, 9: 42, 17: 43, 7: 44, 8: 45, 5: 46, 2: 47, 16: 48}, 'ps_car_07_cat': {1: 49, -1: 50, 0: 51}, 'ps_car_08_cat': {0: 52, 1: 53}, 'ps_car_09_cat': {0: 54, 2: 55, 3: 56, 1: 57, -1: 58, 4: 59}, 'ps_car_10_cat': {1: 60, 0: 61, 2: 62}, 'ps_car_11': {2: 63, 3: 64, 1: 65, 0: 66}, 'ps_car_11_cat': {12: 67, 19: 68, 60: 69, 104: 70, 82: 71, 99: 72, 30: 73, 68: 74, 20: 75, 36: 76, 101: 77, 103: 78, 41: 79, 59: 80, 43: 81, 64: 82, 29: 83, 95: 84, 24: 85, 5: 86, 28: 87, 87: 88, 66: 89, 10: 90, 26: 91, 54: 92, 32: 93, 38: 94, 83: 95, 89: 96, 49: 97, 93: 98, 1: 99, 22: 100, 85: 101, 78: 102, 31: 103, 34: 104, 7: 105, 8: 106, 3: 107, 46: 108, 27: 109, 25: 110, 61: 111, 16: 112, 69: 113, 40: 114, 76: 115, 39: 116, 88: 117, 42: 118, 75: 119, 91: 120, 23: 121, 2: 122, 71: 123, 90: 124, 80: 125, 44: 126, 92: 127, 72: 128, 96: 129, 86: 130, 62: 131, 33: 132, 67: 133, 73: 134, 77: 135, 18: 136, 21: 137, 74: 138, 37: 139, 48: 140, 70: 141, 13: 142, 15: 143, 102: 144, 53: 145, 65: 146, 100: 147, 51: 148, 79: 149, 52: 150, 63: 151, 94: 152, 6: 153, 57: 154, 35: 155, 98: 156, 56: 157, 97: 158, 55: 159, 84: 160, 50: 161, 4: 162, 58: 163, 9: 164, 17: 165, 11: 166, 45: 167, 14: 168, 81: 169, 47: 170}, 'ps_ind_01': {2: 171, 1: 172, 5: 173, 0: 174, 4: 175, 3: 176, 6: 177, 7: 178}, 'ps_ind_02_cat': {2: 179, 1: 180, 4: 181, 3: 182, -1: 183}, 'ps_ind_03': {5: 184, 7: 185, 9: 186, 2: 187, 0: 188, 4: 189, 3: 190, 1: 191, 11: 192, 6: 193, 8: 194, 10: 195}, 'ps_ind_04_cat': {1: 196, 0: 197, -1: 198}, 'ps_ind_05_cat': {0: 199, 1: 200, 4: 201, 3: 202, 6: 203, 5: 204, -1: 205, 2: 206}, 'ps_ind_06_bin': {0: 207, 1: 208}, 'ps_ind_07_bin': {1: 209, 0: 210}, 'ps_ind_08_bin': {0: 211, 1: 212}, 'ps_ind_09_bin': {0: 213, 1: 214}, 'ps_ind_10_bin': {0: 215, 1: 216}, 'ps_ind_11_bin': {0: 217, 1: 218}, 'ps_ind_12_bin': {0: 219, 1: 220}, 'ps_ind_13_bin': {0: 221, 1: 222}, 'ps_ind_14': {0: 223, 1: 224, 2: 225, 3: 226}, 'ps_ind_15': {11: 227, 3: 228, 12: 229, 8: 230, 9: 231, 6: 232, 13: 233, 4: 234, 10: 235, 5: 236, 7: 237, 2: 238, 0: 239, 1: 240}, 'ps_ind_16_bin': {0: 241, 1: 242}, 'ps_ind_17_bin': {1: 243, 0: 244}, 'ps_ind_18_bin': {0: 245, 1: 246}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0320 12:02:17.381104 22320 deprecation.py:506] From <ipython-input-15-2dd8423c9188>:84: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0320 12:02:21.519259 22320 deprecation.py:323] From d:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0320 12:02:21.532225 22320 deprecation.py:506] From d:\\python\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py:187: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#params: 13055\n",
      "6666\n",
      "6666\n",
      "6666\n",
      "6666\n",
      "epoch 0 loss [0.8866888]\n",
      "epoch 1 loss [0.6990677]\n",
      "epoch 2 loss [0.6292895]\n",
      "epoch 3 loss [0.59635246]\n",
      "epoch 4 loss [0.5689062]\n",
      "epoch 5 loss [0.5403135]\n",
      "epoch 6 loss [0.5121026]\n",
      "epoch 7 loss [0.48673117]\n",
      "epoch 8 loss [0.46483436]\n",
      "epoch 9 loss [0.44552055]\n",
      "epoch 10 loss [0.427826]\n",
      "epoch 11 loss [0.4112687]\n",
      "epoch 12 loss [0.3957836]\n",
      "epoch 13 loss [0.38157222]\n",
      "epoch 14 loss [0.36857536]\n",
      "epoch 15 loss [0.35662034]\n",
      "epoch 16 loss [0.3455168]\n",
      "epoch 17 loss [0.33527577]\n",
      "epoch 18 loss [0.32575452]\n",
      "epoch 19 loss [0.31686112]\n",
      "epoch 20 loss [0.30857864]\n",
      "epoch 21 loss [0.30083993]\n",
      "epoch 22 loss [0.29363874]\n",
      "epoch 23 loss [0.28690624]\n",
      "epoch 24 loss [0.2806117]\n",
      "epoch 25 loss [0.2747442]\n",
      "epoch 26 loss [0.2692036]\n",
      "epoch 27 loss [0.26395816]\n",
      "epoch 28 loss [0.25905418]\n",
      "epoch 29 loss [0.2544636]\n",
      "#params: 13055\n",
      "6667\n",
      "6667\n",
      "6667\n",
      "6667\n",
      "epoch 0 loss [0.8799594]\n",
      "epoch 1 loss [0.6906966]\n",
      "epoch 2 loss [0.62569594]\n",
      "epoch 3 loss [0.59938914]\n",
      "epoch 4 loss [0.57192385]\n",
      "epoch 5 loss [0.5399737]\n",
      "epoch 6 loss [0.51081187]\n",
      "epoch 7 loss [0.48665056]\n",
      "epoch 8 loss [0.4658489]\n",
      "epoch 9 loss [0.4467727]\n",
      "epoch 10 loss [0.4289976]\n",
      "epoch 11 loss [0.41270414]\n",
      "epoch 12 loss [0.3978567]\n",
      "epoch 13 loss [0.38418508]\n",
      "epoch 14 loss [0.3714211]\n",
      "epoch 15 loss [0.35962185]\n",
      "epoch 16 loss [0.3487129]\n",
      "epoch 17 loss [0.33861402]\n",
      "epoch 18 loss [0.32924995]\n",
      "epoch 19 loss [0.32054192]\n",
      "epoch 20 loss [0.31234482]\n",
      "epoch 21 loss [0.3047323]\n",
      "epoch 22 loss [0.29767793]\n",
      "epoch 23 loss [0.2910812]\n",
      "epoch 24 loss [0.28483823]\n",
      "epoch 25 loss [0.27911592]\n",
      "epoch 26 loss [0.27353138]\n",
      "epoch 27 loss [0.26843756]\n",
      "epoch 28 loss [0.26368165]\n",
      "epoch 29 loss [0.2591687]\n",
      "#params: 13055\n",
      "6667\n",
      "6667\n",
      "6667\n",
      "6667\n",
      "epoch 0 loss [0.83973855]\n",
      "epoch 1 loss [0.67798436]\n",
      "epoch 2 loss [0.6452861]\n",
      "epoch 3 loss [0.6209167]\n",
      "epoch 4 loss [0.5869973]\n",
      "epoch 5 loss [0.5516073]\n",
      "epoch 6 loss [0.5213077]\n",
      "epoch 7 loss [0.4975137]\n",
      "epoch 8 loss [0.4771632]\n",
      "epoch 9 loss [0.45790297]\n",
      "epoch 10 loss [0.44009984]\n",
      "epoch 11 loss [0.42396575]\n",
      "epoch 12 loss [0.40918377]\n",
      "epoch 13 loss [0.39553407]\n",
      "epoch 14 loss [0.38283956]\n",
      "epoch 15 loss [0.37107927]\n",
      "epoch 16 loss [0.36014596]\n",
      "epoch 17 loss [0.35000965]\n",
      "epoch 18 loss [0.34057027]\n",
      "epoch 19 loss [0.33177644]\n",
      "epoch 20 loss [0.32357767]\n",
      "epoch 21 loss [0.31591067]\n",
      "epoch 22 loss [0.30872515]\n",
      "epoch 23 loss [0.30198672]\n",
      "epoch 24 loss [0.295671]\n",
      "epoch 25 loss [0.28971738]\n",
      "epoch 26 loss [0.28407952]\n",
      "epoch 27 loss [0.27883646]\n",
      "epoch 28 loss [0.27394372]\n",
      "epoch 29 loss [0.26927057]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def load_data():\n",
    "    dfTrain = pd.read_csv(TRAIN_FILE)\n",
    "    dfTest = pd.read_csv(TEST_FILE)\n",
    "\n",
    "    def preprocess(df):\n",
    "        cols = [c for c in df.columns if c not in [\"id\", \"target\"]]\n",
    "        df[\"missing_feat\"] = np.sum((df[cols] == -1).values, axis=1)\n",
    "        df[\"ps_car_13_x_ps_reg_03\"] = df[\"ps_car_13\"] * df[\"ps_reg_03\"]\n",
    "        return df\n",
    "\n",
    "    dfTrain = preprocess(dfTrain)\n",
    "    dfTest = preprocess(dfTest)\n",
    "\n",
    "    cols = [c for c in dfTrain.columns if c not in [\"id\", \"target\"]]\n",
    "    cols = [c for c in cols if (not c in IGNORE_COLS)]\n",
    "\n",
    "    X_train = dfTrain[cols].values\n",
    "    y_train = dfTrain[\"target\"].values\n",
    "    X_test = dfTest[cols].values\n",
    "    ids_test = dfTest[\"id\"].values\n",
    "\n",
    "    return dfTrain, dfTest, X_train, y_train, X_test, ids_test,\n",
    "\n",
    "\n",
    "def run_base_model_dcn(dfTrain, dfTest, folds, dcn_params):\n",
    "\n",
    "    fd = FeatureDictionary(dfTrain,dfTest,numeric_cols=NUMERIC_COLS,\n",
    "                           ignore_cols=IGNORE_COLS,\n",
    "                           cate_cols = CATEGORICAL_COLS)\n",
    "\n",
    "    print(fd.feat_dim)\n",
    "    print(fd.feat_dict)\n",
    "\n",
    "    data_parser = DataParser(feat_dict=fd)\n",
    "    cate_Xi_train, cate_Xv_train, numeric_Xv_train,y_train = data_parser.parse(df=dfTrain, has_label=True)\n",
    "    cate_Xi_test, cate_Xv_test, numeric_Xv_test,ids_test = data_parser.parse(df=dfTest)\n",
    "\n",
    "    dcn_params[\"cate_feature_size\"] = fd.feat_dim\n",
    "    dcn_params[\"field_size\"] = len(cate_Xi_train[0])\n",
    "    dcn_params['numeric_feature_size'] = len(NUMERIC_COLS)\n",
    "\n",
    "    _get = lambda x, l: [x[i] for i in l]\n",
    "\n",
    "    for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "        cate_Xi_train_, cate_Xv_train_, numeric_Xv_train_,y_train_ = _get(cate_Xi_train, train_idx), _get(cate_Xv_train, train_idx),_get(numeric_Xv_train, train_idx), _get(y_train, train_idx)\n",
    "        cate_Xi_valid_, cate_Xv_valid_, numeric_Xv_valid_,y_valid_ = _get(cate_Xi_train, valid_idx), _get(cate_Xv_train, valid_idx),_get(numeric_Xv_train, valid_idx), _get(y_train, valid_idx)\n",
    "\n",
    "        dcn =  DCN(**dcn_params)\n",
    "\n",
    "        dcn.fit(cate_Xi_train_, cate_Xv_train_, numeric_Xv_train_,y_train_, cate_Xi_valid_, cate_Xv_valid_, numeric_Xv_valid_,y_valid_)\n",
    "\n",
    "#dfTrain = pd.read_csv(config.TRAIN_FILE,nrows=10000,index_col=None).to_csv(config.TRAIN_FILE,index=False)\n",
    "#dfTest = pd.read_csv(config.TEST_FILE,nrows=2000,index_col=None).to_csv(config.TEST_FILE,index=False)\n",
    "\n",
    "dfTrain, dfTest, X_train, y_train, X_test, ids_test = load_data()\n",
    "print('load_data_over')\n",
    "folds = list(StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True,\n",
    "                             random_state=RANDOM_SEED).split(X_train, y_train))\n",
    "print('process_data_over')\n",
    "\n",
    "dcn_params = {\n",
    "\n",
    "    \"embedding_size\": 8,\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"deep_layers_activation\": tf.nn.relu,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"verbose\": True,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"cross_layer_num\":3\n",
    "}\n",
    "print('start train')\n",
    "run_base_model_dcn(dfTrain, dfTest, folds, dcn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
